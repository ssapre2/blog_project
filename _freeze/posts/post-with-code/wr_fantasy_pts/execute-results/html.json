{
  "hash": "f58b1e89c387d512224cfe029e41b690",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: \"Predicting Wide Receiver Fantasy Points w/ Tidymodels\"\nauthor: \"Sameer Sapre\"\ndate: \"2024-05-16\"\ncategories: [news, code, analysis]\nimage: \"chase.jpg\"\ndraft: TRUE\neditor: visual\n---\n\n\n\n\n## Introduction\n\nI've played fantasy football for a decade and have always wondered how ESPN (and Yahoo, Sleeper, etc.) make their fantasy projections week to week. Where are they getting their numbers? Where are they getting probabilities from? What makes them think that Ja'Marr Chase is going to score 16.3 points for me this week vs. D? Why does his projection go up that much (or that little) if Tee Higgins happens to be out injured? Perhaps we won't get a chance to peak inside of ESPN's crystal ball, but maybe we can try to build our own? \n\nIn this post, I'll attempt to build a predictive model that can output our projections for fantasy football players based on their past performance. We'll narrow the scope to wide receivers now, but this process can largely be applied to players of other positions. \n\nWe'll go step by step through data cleaning, feature engineering, model selection and training, and model validation before finishing up with conclusions and takeaways.\n\n## R Markdown\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(tidyverse)\nlibrary(nflreadr)\nlibrary(tidymodels)\nlibrary(nflfastR)\nlibrary(naniar)\n```\n:::\n\n\n## Data Load\n\nWe'll start by loading in directly relevant player statistics and depth chart information before joining them together.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nstats = load_player_stats(seasons = seq(2006,2023))\n\ndc = load_depth_charts(season = seq(2016,most_recent_season())) %>% filter(position == 'WR',formation == 'Offense') %>%\n  select(season,recent_team = club_code,week,season_type = game_type,player_id = gsis_id,depth_team)\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\n# Filter for wide receiver plays\nwr_data <- stats %>%\n  filter(position == \"WR\") %>%\n  # Select identifying info + receiver specfic metrics\n  select(player_id,player_name,position,recent_team,season,week,season_type,\n         receptions:fantasy_points_ppr) %>%\n  # Add depth chart status since we don't have participation data\n  left_join(y = dc,by = c('player_id','week','season','season_type',\"recent_team\")) %>%\n  # Only first 3 are counted so some players are NA'd if they're below 3rd on DC\n  replace_na(list(depth_team = '4')) %>%\n  mutate(depth_team = as.numeric(depth_team))\n```\n:::\n\n\n\n## Missingness\n\nLet's check if there are any missing values in the dataset we've created. We'll use the library `naniar` to do so.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Explore variables patterns of missingness\ngg_miss_var(wr_data)\n```\n\n::: {.cell-output-display}\n![](wr_fantasy_pts_files/figure-html/unnamed-chunk-4-1.png){width=672}\n:::\n\n```{.r .cell-code}\nwr_data %>%\n  select(racr,air_yards_share,wopr,target_share,receiving_epa) -> df_miss\n\ndf_miss %>%\n  gg_miss_upset()\n```\n\n::: {.cell-output-display}\n![](wr_fantasy_pts_files/figure-html/unnamed-chunk-4-2.png){width=672}\n:::\n\n```{.r .cell-code}\n# What if we take out racr?\n```\n:::\n\n\n Ideally, we'd could probably impute these values, but for the brevity, we'll filter out the NA's of racr, due to the intersection with other metrics. Player Name is not important for our purposes.\n\n## Time Weighting\n\nWe want our predictions for each player to be based on their past performance. However, we'd like the most recent performances to be weighed the heaviest. To do that, we can introduce a simple time-weighting scheme. In the function below, we take a data vector (ex. receptions) and return a weighted average based on the length of the vector.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Create Time Weighting \n\nweighted_avg = function(metric_vector){\n  \n  # Take in sliding window of vector of chosen metric\n  n = length(metric_vector)\n\n  # Create Weights for each value based on recency\n  weights = seq(1,n)\n  \n  # Calculated weighted average\n  w_avg = sum(metric_vector * weights) / sum(weights)\n  \n  return(w_avg)\n\n}\n```\n:::\n\n\nNow that we have our simple time-weighting built, let's make use of Davis Vaughn's [slider] (https://slider.r-lib.org/articles/slider.html) package. It's a tidy way to calculate sliding window summaries. We'll do that to create a set of potential numeric predictors.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(TTR)\n```\n\n::: {.cell-output .cell-output-stderr}\n\n```\n\nAttaching package: 'TTR'\n```\n\n\n:::\n\n::: {.cell-output .cell-output-stderr}\n\n```\nThe following object is masked from 'package:dials':\n\n    momentum\n```\n\n\n:::\n\n```{.r .cell-code}\nlibrary(slider)\n\nwr_data %>%\n  # Should remove most missingness\n  filter(!is.na(racr)) %>%\n  group_by(player_id,season) %>%\n  arrange(week) %>%\n  # Weighted Avg (moving)\n  # Take lag so we are not leaking any data\n  mutate(across(receptions:depth_team, ~ lag(slide_dbl(.x,.f = weighted_avg,.before = Inf,.complete = TRUE)),.names = \"wt_{col}\")) %>%\n  ungroup() %>%\n  # Convert negative fantasy points to 0\n  mutate(fantasy_points_target = ifelse(fantasy_points < 0,0,fantasy_points),\n         log_fantasy_points = log(fantasy_points_target + 1)) -> ma_wr\n```\n:::\n\n\nLet's use the example of Brandon Aiyuk as a sanity check.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nma_wr %>%\n  filter(season == 2023, player_id == '00-0036261') %>%\n  select(player_name,week,targets,wt_targets, receptions,wt_receptions, receiving_yards,wt_receiving_yards)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 19 × 8\n   player_name  week targets wt_targets receptions wt_receptions receiving_yards\n   <chr>       <int>   <int>      <dbl>      <int>         <dbl>           <dbl>\n 1 B.Aiyuk         1       8      NA             8         NA                129\n 2 B.Aiyuk         2       6       8             3          8                 43\n 3 B.Aiyuk         4       6       6.67          6          4.67             148\n 4 B.Aiyuk         5       7       6.33          4          5.33              58\n 5 B.Aiyuk         6      10       6.6           4          4.8               76\n 6 B.Aiyuk         7       6       7.73          5          4.53              57\n 7 B.Aiyuk         8       9       7.24          5          4.67             109\n 8 B.Aiyuk        10       3       7.68          3          4.75              55\n 9 B.Aiyuk        11       6       6.64          5          4.36             156\n10 B.Aiyuk        12       4       6.51          2          4.49              50\n11 B.Aiyuk        13       7       6.05          5          4.04              46\n12 B.Aiyuk        14       9       6.21          6          4.20             126\n13 B.Aiyuk        15       5       6.64          3          4.47              37\n14 B.Aiyuk        16       7       6.41          6          4.26             113\n15 B.Aiyuk        17       8       6.49          7          4.50             114\n16 B.Aiyuk        18       4       6.68          3          4.81              25\n17 B.Aiyuk        20       6       6.36          3          4.60              32\n18 B.Aiyuk        21       8       6.32          3          4.42              68\n19 B.Aiyuk        22       6       6.50          3          4.27              49\n# ℹ 1 more variable: wt_receiving_yards <dbl>\n```\n\n\n:::\n:::\n\nA quick scan indicates that our time weighted average is calculated as intended.\n\n\n## Correlations\n\nBefore we start splitting data for the model, let's take a look at variable correlations. This may influence our model choices.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Calculate the correlation matrix\ncor_matrix <- ma_wr %>% \n  select(starts_with(\"wt_\"), fantasy_points_target) %>%\n  cor(use = \"complete.obs\")\n\n# Reshape the correlation matrix for ggplot\ncor_data <- reshape2::melt(cor_matrix)\n\nggplot(data = cor_data, aes(x = Var1, y = Var2, fill = value)) +\n  geom_tile(color = \"white\") +\n  scale_fill_gradient2(low = \"blue\", high = \"red\", mid = \"white\", midpoint = 0, limit = c(-1,1), name=\"Correlation\") +\n  theme_minimal() +\n  theme(axis.text.x = element_text(angle = 45, vjust = 1, hjust = 1)) +\n  labs(title = \"Correlation Matrix of Dataset\", x = \"\", y = \"\")\n```\n\n::: {.cell-output-display}\n![](wr_fantasy_pts_files/figure-html/unnamed-chunk-8-1.png){width=672}\n:::\n:::\n\n\nOkay, it looks like there is a high degree of colinearity between our weighted measures. Not very surprising. We'll use a few modeling techniques below that should address the issues of highly correlated variables.\n\n\nIn addition, if we look at our target variable, we notice something interesting... it's got a skewed distribution. Should we be using linear regression? \n\n\n::: {.cell}\n\n```{.r .cell-code}\nma_wr %>%\n  ggplot(aes(x = fantasy_points_target)) +\n    geom_histogram()\n```\n\n::: {.cell-output .cell-output-stderr}\n\n```\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n```\n\n\n:::\n\n::: {.cell-output-display}\n![](wr_fantasy_pts_files/figure-html/unnamed-chunk-9-1.png){width=672}\n:::\n:::\n\n\nLet's see what happens if we transform our target with a log-transform.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nma_wr %>%\n  ggplot(aes(x = log_fantasy_points)) +\n  geom_histogram()\n```\n\n::: {.cell-output .cell-output-stderr}\n\n```\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n```\n\n\n:::\n\n::: {.cell-output-display}\n![](wr_fantasy_pts_files/figure-html/unnamed-chunk-10-1.png){width=672}\n:::\n:::\n\n\nHmmm, our histogram still looks a bit funky. The distribution seems to have shifted closer to a bell curve we'd want for linear regression, but there are A LOT of 0s (more on that later).\n\n## Preprocess\n\nWe'll start splitting our dataset into training and testing splits and create recipes for preprocessing in the following blocks.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(tidymodels)\n\n# Split\n\nset.seed(222)\n# Put 3/4 of the data into the training set \ndata_split <- ma_wr %>% \n  # Filter on relevant columns\n  select(starts_with('wt_'),fantasy_points_target,player_id,season,week,recent_team,\n         fantasy_points,fantasy_points_ppr,log_fantasy_points) %>% \n  # make split\n  initial_split( prop = 3/4)\n\n# Create data frames for the two sets:\ntrain_data <- training(data_split)\ntest_data  <- testing(data_split)\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nwt_recipe = train_data %>%\n  recipe(fantasy_points_target ~ .,) %>%\n  update_role(c(player_id,recent_team,fantasy_points,fantasy_points_ppr),new_role = 'ID') %>%\n  # Generally not recommended to throw out all data, but for brevity, let's remove NAs\n  #step_naomit(all_numeric_predictors()) %>%\n  step_impute_median(all_numeric_predictors()) %>%\n  # Remove zero variance predictors (ie. variables that contribute nothing to prediction)\n step_center(all_numeric_predictors())\n#\n\nsummary(wt_recipe)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 27 × 4\n   variable                       type      role      source  \n   <chr>                          <list>    <chr>     <chr>   \n 1 wt_receptions                  <chr [2]> predictor original\n 2 wt_targets                     <chr [2]> predictor original\n 3 wt_receiving_yards             <chr [2]> predictor original\n 4 wt_receiving_tds               <chr [2]> predictor original\n 5 wt_receiving_fumbles           <chr [2]> predictor original\n 6 wt_receiving_fumbles_lost      <chr [2]> predictor original\n 7 wt_receiving_air_yards         <chr [2]> predictor original\n 8 wt_receiving_yards_after_catch <chr [2]> predictor original\n 9 wt_receiving_first_downs       <chr [2]> predictor original\n10 wt_receiving_epa               <chr [2]> predictor original\n# ℹ 17 more rows\n```\n\n\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\ntest_data %>%\n  filter(!is.na(wt_receptions)) -> test_data\n\nsum(colSums(is.na(test_data)))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 0\n```\n\n\n:::\n:::\n\n\n## Model Building -------------------------------\n\n\n::: {.cell}\n\n```{.r .cell-code}\nexp_recipe = train_data  %>%\n  recipe(log_fantasy_points ~ .,) %>%\n  update_role(c(player_id,recent_team,fantasy_points,fantasy_points_ppr,fantasy_points_target),new_role = 'ID') %>%\n  # Generally not recommended to throw out all data, but for brevity, let's remove NAs\n  #step_naomit(all_numeric_predictors()) %>%\n  step_impute_median(all_numeric_predictors()) %>%\n\n # Remove zero variance predictors (ie. variables that contribute nothing to prediction)\n  step_zv(all_predictors()) %>%\n  step_center(all_numeric_predictors())\n\n\n#summary(exp_recipe)\n```\n:::\n\n\n# Cross-Validation + Tuning -------------\n\n## Create Model\n\nCreate specifications for elastic net model using glmnet and tune\n\n## GLM --------------------------\n\nWe'll train a general linear model (GLM). This is a type of linear regression which provides a type of built-in variable selection. The nuts and bolts of this modeling strategy are beyond the scope of this post, but for those interested in learning, there's tons of material on a concept called *regularization* upon which this strategy is built.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(glmnet)\n```\n\n::: {.cell-output .cell-output-stderr}\n\n```\nLoading required package: Matrix\n```\n\n\n:::\n\n::: {.cell-output .cell-output-stderr}\n\n```\n\nAttaching package: 'Matrix'\n```\n\n\n:::\n\n::: {.cell-output .cell-output-stderr}\n\n```\nThe following objects are masked from 'package:tidyr':\n\n    expand, pack, unpack\n```\n\n\n:::\n\n::: {.cell-output .cell-output-stderr}\n\n```\nLoaded glmnet 4.1-8\n```\n\n\n:::\n\n```{.r .cell-code}\n#library(lightgbm)\n#library(bonsai)\n\nglm_spec <- linear_reg(\n  penalty = tune(),     # Lambda (regularization strength)\n  mixture = tune(),    # Alpha (0 = Ridge, 1 = Lasso, values in between = Elastic Net)\n    \n) %>%\n  set_engine(\"glmnet\")\n\nglm_wflow <-\n  workflow() %>% \n  add_recipe(exp_recipe) %>%\n  add_model(glm_spec)\n\n\nwr_folds <- vfold_cv(train_data, v = 5)\n\n# Tune the hyperparameters using a grid of values\nglm_tune_results <- tune_grid(\n  glm_wflow,\n  resamples = wr_folds,\n  grid = 10   # Number of tuning combinations to evaluate\n)\n```\n:::\n\n\nTune Model w/ Cross Validation\n\nExamine Tuning Results\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Display tuning results\n\nglm_tune_results %>%\n  collect_metrics() %>%\n  filter(.metric == \"rmse\") %>%\n  arrange(mean)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 10 × 8\n    penalty mixture .metric .estimator  mean     n std_err .config              \n      <dbl>   <dbl> <chr>   <chr>      <dbl> <int>   <dbl> <chr>                \n 1 2.00e- 7  0.574  rmse    standard   0.799     5 0.00334 Preprocessor1_Model06\n 2 1.36e- 6  0.840  rmse    standard   0.799     5 0.00334 Preprocessor1_Model09\n 3 2.26e- 5  0.352  rmse    standard   0.799     5 0.00336 Preprocessor1_Model04\n 4 2.32e- 9  0.499  rmse    standard   0.799     5 0.00333 Preprocessor1_Model05\n 5 1.53e- 8  0.0923 rmse    standard   0.799     5 0.00335 Preprocessor1_Model01\n 6 2.89e-10  0.801  rmse    standard   0.799     5 0.00333 Preprocessor1_Model08\n 7 4.89e- 4  0.909  rmse    standard   0.799     5 0.00333 Preprocessor1_Model10\n 8 9.91e- 3  0.651  rmse    standard   0.800     5 0.00331 Preprocessor1_Model07\n 9 5.00e- 2  0.237  rmse    standard   0.800     5 0.00332 Preprocessor1_Model02\n10 3.95e- 1  0.326  rmse    standard   0.816     5 0.00272 Preprocessor1_Model03\n```\n\n\n:::\n:::\n\n\nFind the best parameters of the group. `finalize_workflow()` will choose the model with the optimal set of hyperparameters as found in `select_best()`.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Select the best hyperparameters based on RMSE\nbest_glm <- select_best(glm_tune_results, metric = 'rmse')\n\n# Finalize the workflow with the best hyperparameters\nfinal_glm_workflow <- finalize_workflow(glm_wflow, best_glm)\n\nbest_glm\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 1 × 3\n      penalty mixture .config              \n        <dbl>   <dbl> <chr>                \n1 0.000000200   0.574 Preprocessor1_Model06\n```\n\n\n:::\n:::\n\n\n\\^ Above you'll find the optimal configuration of the model. We won't get too far into the weeds here, but will not that the penalty term is small (low regularization, high complexity) and the mixture (type of regularization) indicates more of a Ridge regression (more shrinkage, less variable elimination). \n\nBelow, we'll take the best model and fit it to the entire training data set before validating it against the test set.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Fit the finalized model on the entire training data\nfinal_glm_fit <- fit(final_glm_workflow, data = train_data)\n```\n:::\n\n\n\n## Model Evaluation\n\n### Diagnostics\n\nNow that we have our model fit on the full training set, let's evaluate it, check it's reliability and it's compliance with key assumptions.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Make predictions on the test set and tidy\nglm_predictions <- augment(final_glm_fit, new_data = test_data) %>%\n  mutate(.pred_fp = exp(.pred) + 1,\n         .resid = fantasy_points - .pred_fp)\n\n# Evaluate the model's performance (RMSE)\nglm_metrics <- glm_predictions %>%\n  metrics(truth = fantasy_points, estimate = .pred_fp)\n\n# Print the evaluation metrics\nprint(glm_metrics)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 3 × 3\n  .metric .estimator .estimate\n  <chr>   <chr>          <dbl>\n1 rmse    standard       5.45 \n2 rsq     standard       0.148\n3 mae     standard       4.21 \n```\n\n\n:::\n\n```{.r .cell-code}\n# Distribution of predictors\nglm_predictions  %>%\n  ggplot(aes(x = .resid)) +\n    geom_histogram()\n```\n\n::: {.cell-output .cell-output-stderr}\n\n```\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n```\n\n\n:::\n\n::: {.cell-output-display}\n![](wr_fantasy_pts_files/figure-html/unnamed-chunk-19-1.png){width=672}\n:::\n\n```{.r .cell-code}\n# Residuals vs fitted\nggplot(glm_predictions,aes(x = .pred_fp, y=.resid)) +\n  geom_point() +\n  geom_hline(yintercept = 0,linetype = \"dashed\",color = \"red\")+\n  labs(title = \"Residuals vs. Fitted\", y= \"Residuals\",x = \"Fitted\")\n```\n\n::: {.cell-output-display}\n![](wr_fantasy_pts_files/figure-html/unnamed-chunk-19-2.png){width=672}\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nggplot(glm_predictions,aes(sample = .resid)) +\n  geom_qq() +\n  geom_qq_line() +\n  labs(title = \"Q-Q Plot of Residuals\") +\n  theme_minimal() \n```\n\n::: {.cell-output-display}\n![](wr_fantasy_pts_files/figure-html/unnamed-chunk-20-1.png){width=672}\n:::\n:::\n\n\nThis plot is a bit indicting as we can see the points at the end of the QQPlot start to tail off. It suggests that there are some outliers in fantasy points scored and/or a linear model may not be the best approach for this dataset. This lines up with the inflated number of 0s seen in our target variable. We probably should not trust this model for valid inference or prediction and should continue to explore non-linear options or modifications to this model (i.e. variable interactions or transformations).  \n\n\n### Variable Importance\n\n\n::: {.cell}\n\n```{.r .cell-code}\nfinal_glm_fit %>% \n  vip::vi() %>%\n  mutate(Importance = abs(Importance),\n         Variable = fct_reorder(Variable,Importance)) %>%\n  ggplot(aes(x = Importance, y = Variable, fill = Sign)) +\n  geom_col() +\n  scale_x_continuous(expand = c(0,0)) +\n  labs(x = \"Impact\")\n```\n\n::: {.cell-output-display}\n![](wr_fantasy_pts_files/figure-html/unnamed-chunk-21-1.png){width=672}\n:::\n:::\n\n\nI'm not personally not a huge fan of variable importance plots because they don't communicate the actual \"importance\" of the variable to the outcome of our target. In other words, they're not that useful (by themselves) for inference. They really just measure the impact of the variables on predictions. I'll take a few variables on this plot as examples.\n\nIn this case, we can see that \"wt_target_share\" is defined as \"Player's share of team receiving targets in this game\".\n\nIt makes sense for this to have a positive impact on fantasy points scored as it directly describes fantasy opportunity for receivers. All else being equal, it's likely that increased opportunity results in increased fantasy point value and we can reasonably say something like, \"If a WR sees an increase in their opportuntiy rate due to increased skill, trust from coaches, injury to teammates, etc. we can can expect an increase in fantasy production\". \n\nHowever is \"receiving_fumbles_lost\", the second highest impact, reasonable? Considering that fantasy points are **deducted** when a player fumbles the ball in real life, this doesn't make much sense. We'd be hard-pressed to convince anyone to target a player who fumbles the ball often. Why is this the case?\n\nIt's likely due to an issue called collinearity. In order to fumble the ball you must first have **received it**. The players with the highest number of fumbles are likely the ones getting the ball thrown to their way the most and consequently getting increased opportunity to score more points in addition to fumbles. This is a prime example of an issue called multi-collinearity or a fancy way of saying that 2 predictors are highly correlated with each other. Below we can see \"fumbles lost\" as a function of receptions and target share, respectfully.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nggplot(train_data, aes(x = wt_receptions, y = wt_receiving_fumbles_lost)) + \n  geom_smooth() +\n  theme_minimal() +\n  labs(y = \"Fumbles Lost\", x = \"Receptions\")\n```\n\n::: {.cell-output .cell-output-stderr}\n\n```\n`geom_smooth()` using method = 'gam' and formula = 'y ~ s(x, bs = \"cs\")'\n```\n\n\n:::\n\n::: {.cell-output .cell-output-stderr}\n\n```\nWarning: Removed 2693 rows containing non-finite outside the scale range\n(`stat_smooth()`).\n```\n\n\n:::\n\n::: {.cell-output-display}\n![](wr_fantasy_pts_files/figure-html/unnamed-chunk-22-1.png){width=672}\n:::\n\n```{.r .cell-code}\nggplot(train_data, aes(x = wt_target_share, y = wt_receiving_fumbles_lost)) + \n  geom_smooth() +\n  labs(y = \"Fumbles\", x= \"Target Share\")\n```\n\n::: {.cell-output .cell-output-stderr}\n\n```\n`geom_smooth()` using method = 'gam' and formula = 'y ~ s(x, bs = \"cs\")'\n```\n\n\n:::\n\n::: {.cell-output .cell-output-stderr}\n\n```\nWarning: Removed 2693 rows containing non-finite outside the scale range\n(`stat_smooth()`).\n```\n\n\n:::\n\n::: {.cell-output-display}\n![](wr_fantasy_pts_files/figure-html/unnamed-chunk-22-2.png){width=672}\n:::\n:::\n\n\n\n\nAs mentioned earlier, the regularized model handles variable selection but does not completely remove the effects of multicollinearity. Multicollinearity makes inference very challenging and we will likely need a model better suited for non-linear relationships or to include more interactions to improve model performance and interpretability.\n\n## Conclusion\n\nThis was a good first step in creating a model for my fantasy football needs. There is clearly room for improvement as far as model selection goes, but it was fun to use the `slider` package for the first time and attempt to a transformed target variable. I have some ideas for additions to and subtractions from this model that will hopefully make it into another blog post. In the meantime, big shoutout to the `nflreadr` team for making this data easily available. \n\n@Manual{,\n  title = {nflreadr: Download 'nflverse' Data},\n  author = {Tan Ho and Sebastian Carl},\n  year = {2024},\n  note = {R package version 1.4.1.05, https://github.com/nflverse/nflreadr},\n  url = {https://nflreadr.nflverse.com},\n}\n\n",
    "supporting": [
      "wr_fantasy_pts_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}