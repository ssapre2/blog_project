{
  "hash": "15998564ee00d24e9475e0868d08157a",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: \"Predicting Wide Receiver Fantasy Points w/ slideR and Tidymodels\"\nauthor: \"Sameer Sapre\"\neditor: visual\n---\n\n\n\n\n## R Markdown\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(tidyverse)\nlibrary(nflreadr)\nlibrary(tidymodels)\nlibrary(nflfastR)\n```\n:::\n\n\n## Data Load\n\n\n::: {.cell}\n\n```{.r .cell-code}\nstats24 = load_player_stats(2024)\nstats = load_player_stats(seasons = seq(2006,2023))\npbp23 = load_pbp(2023)\n\n\ndc = load_depth_charts(season = seq(2016,most_recent_season())) %>% filter(position == 'WR',formation == 'Offense') %>%\n  select(season,recent_team = club_code,week,season_type = game_type,player_id = gsis_id,depth_team)\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\n# Filter for wide receiver plays\nwr_data <- stats %>%\n  filter(position == \"WR\") %>%\n  select(player_id,player_name,position,recent_team,season,week,season_type,\n         receptions:fantasy_points_ppr) %>%\n  # Receiving FP\n  mutate(rec_fp = (receiving_yards * 0.1) + (receiving_tds * 6) + (receptions * 0.5)) %>%\n  # Add depth chart status since we don't have participation data\n  left_join(y = dc,by = c('player_id','week','season','season_type',\"recent_team\")) %>%\n  # Only first 3 are counted so some players are NA'd if they're below 3rd on DC\n  replace_na(list(depth_team = '4')) %>%\n  mutate(depth_team = as.numeric(depth_team))\n```\n:::\n\n\nNumber of Snaps by offense ---------\n\nFFO (Comparison)\n\n\n::: {.cell}\n\n```{.r .cell-code}\nread.csv('https://github.com/ffverse/ffopportunity/releases/download/latest-data/ep_weekly_2024.csv') -> ff_2024\n\nff_2024 %>% filter(position == 'WR') %>% select(game_id,player_id,season,rec_fantasy_points,rec_fantasy_points_exp) %>%\n  metrics(truth = rec_fantasy_points,estimate = rec_fantasy_points_exp)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 3 × 3\n  .metric .estimator .estimate\n  <chr>   <chr>          <dbl>\n1 rmse    standard       4.57 \n2 rsq     standard       0.629\n3 mae     standard       3.28 \n```\n\n\n:::\n:::\n\n\n## Missingness\n\nIt looks like there are some patterns of missingness in the dataset we've created. Ideally, we'd could probably impute these values, but for the brevity, we'll filter out the NA's of racr, due to the intersection with other metrics. \n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(naniar)\n\ngg_miss_case(wr_data)\n```\n\n::: {.cell-output-display}\n![](wr_fantasy_pts_files/figure-html/unnamed-chunk-5-1.png){width=672}\n:::\n\n```{.r .cell-code}\ngg_miss_var(wr_data)\n```\n\n::: {.cell-output-display}\n![](wr_fantasy_pts_files/figure-html/unnamed-chunk-5-2.png){width=672}\n:::\n\n```{.r .cell-code}\nwr_data %>%\n  select(racr,air_yards_share,wopr,target_share,receiving_epa) -> df_miss\n\ndf_miss %>%\n  gg_miss_upset()\n```\n\n::: {.cell-output-display}\n![](wr_fantasy_pts_files/figure-html/unnamed-chunk-5-3.png){width=672}\n:::\n\n```{.r .cell-code}\n# What if we take out racr?\n```\n:::\n\n\n\n## Time Weighting\n\nWe want our predictions for each player to be based on their past performance. Ideally, we'd like the most recent performances to be weighed the heaviest. To do that, we can introduce a simple time-weighting scheme. In the function below, we take a data vector (ex. receptions) and return a weighted average based on the length of the vector.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Create Time Weighting \n\nweighted_avg = function(metric_vector){\n  \n  # Take in sliding window of vector of chosen metric\n  n = length(metric_vector)\n\n  # Create Weights for each value based on recency\n  weights = seq(1,n)\n  \n  # Calculated weighted average\n  w_avg = sum(metric_vector * weights) / sum(weights)\n  \n  return(w_avg)\n\n}\n```\n:::\n\n\nNow that we have our simple time-weighting built, let's make use of Davis Vaughn's [slider] (https://slider.r-lib.org/articles/slider.html) package. It's a tidy way to calculate sliding window summaries. We'll do that to create a set of potential numeric predictors.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(TTR)\n```\n\n::: {.cell-output .cell-output-stderr}\n\n```\n\nAttaching package: 'TTR'\n```\n\n\n:::\n\n::: {.cell-output .cell-output-stderr}\n\n```\nThe following object is masked from 'package:dials':\n\n    momentum\n```\n\n\n:::\n\n```{.r .cell-code}\nlibrary(slider)\n\nwr_data %>%\n  # Should remove most missingness\n  filter(!is.na(racr)) %>%\n  group_by(player_id,season) %>%\n  arrange(week) %>%\n  # Weighted Avg (moving)\n  # Take lag so we are not leaking any data\n  mutate(across(receptions:depth_team, ~ lag(slide_dbl(.x,.f = weighted_avg,.before = Inf,.complete = TRUE)),.names = \"wt_{col}\")) %>%\n  ungroup() %>%\n  # Convert negative fantasy points to 0\n  mutate(fantasy_points_target = ifelse(fantasy_points < 0,0,fantasy_points),\n         log_fantasy_points = log(fantasy_points_target + 1)) %>%\n  # Need data, don't use week 1 for now\n  filter(week > 1) -> ma_wr\n```\n:::\n\n\n\n\n## Correlations\n\nBefore we start splitting data for the model, let's take a look at variable correlations. This may influence our model choices.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Calculate the correlation matrix\ncor_matrix <- ma_wr %>% \n  select(starts_with(\"wt_\"), fantasy_points_target) %>%\n  cor(use = \"complete.obs\")\n\n# Reshape the correlation matrix for ggplot\ncor_data <- reshape2::melt(cor_matrix)\n\nggplot(data = cor_data, aes(x = Var1, y = Var2, fill = value)) +\n  geom_tile(color = \"white\") +\n  scale_fill_gradient2(low = \"blue\", high = \"red\", mid = \"white\", midpoint = 0, limit = c(-1,1), name=\"Correlation\") +\n  theme_minimal() +\n  theme(axis.text.x = element_text(angle = 45, vjust = 1, hjust = 1)) +\n  labs(title = \"Correlation Matrix of Dataset\", x = \"\", y = \"\")\n```\n\n::: {.cell-output-display}\n![](wr_fantasy_pts_files/figure-html/unnamed-chunk-8-1.png){width=672}\n:::\n:::\n\n\nOkay, it looks like there is a high degree of colinearity between our weighted measures. Not very surprising. We'll use a few modeling techniques below that should address the issues of highly correlated variables.\n\n\nIn addition, if we look at our target variable, we notice something interesting... it's got a funky distribution. Should we be using linear regression?  \n\n\n::: {.cell}\n\n```{.r .cell-code}\nma_wr %>%\n  ggplot(aes(x = fantasy_points_target)) +\n    geom_histogram()\n```\n\n::: {.cell-output .cell-output-stderr}\n\n```\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n```\n\n\n:::\n\n::: {.cell-output-display}\n![](wr_fantasy_pts_files/figure-html/unnamed-chunk-9-1.png){width=672}\n:::\n\n```{.r .cell-code}\nma_wr %>%\n  ggplot(aes(x = log_fantasy_points)) +\n  geom_histogram()\n```\n\n::: {.cell-output .cell-output-stderr}\n\n```\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n```\n\n\n:::\n\n::: {.cell-output-display}\n![](wr_fantasy_pts_files/figure-html/unnamed-chunk-9-2.png){width=672}\n:::\n:::\n\n\n\n\n## Preprocess\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(tidymodels)\n\n# Split\n\nset.seed(222)\n# Put 3/4 of the data into the training set \ndata_split <- ma_wr %>% \n  # Don't use first week\n  filter(week >1)%>%\n  # Filter on relevant columns\n  select(starts_with('wt_'),fantasy_points_target,player_id,season,week,recent_team,\n         fantasy_points,fantasy_points_ppr,log_fantasy_points) %>% \n  # make split\n  initial_split( prop = 3/4)\n\n# Create data frames for the two sets:\ntrain_data <- training(data_split)\ntest_data  <- testing(data_split)\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nwt_recipe = train_data %>%\n  recipe(fantasy_points_target ~ .,) %>%\n  update_role(c(player_id,recent_team,fantasy_points,fantasy_points_ppr),new_role = 'ID') %>%\n  # Generally not recommended to throw out all data, but for brevity, let's remove NAs\n  step_naomit(all_numeric_predictors()) %>%\n  # Remove zero variance predictors (ie. variables that contribute nothing to prediction)\n step_center(all_numeric_predictors())\n#\n\nsummary(wt_recipe)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 28 × 4\n   variable                       type      role      source  \n   <chr>                          <list>    <chr>     <chr>   \n 1 wt_receptions                  <chr [2]> predictor original\n 2 wt_targets                     <chr [2]> predictor original\n 3 wt_receiving_yards             <chr [2]> predictor original\n 4 wt_receiving_tds               <chr [2]> predictor original\n 5 wt_receiving_fumbles           <chr [2]> predictor original\n 6 wt_receiving_fumbles_lost      <chr [2]> predictor original\n 7 wt_receiving_air_yards         <chr [2]> predictor original\n 8 wt_receiving_yards_after_catch <chr [2]> predictor original\n 9 wt_receiving_first_downs       <chr [2]> predictor original\n10 wt_receiving_epa               <chr [2]> predictor original\n# ℹ 18 more rows\n```\n\n\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\ntest_data %>%\n  filter(!is.na(wt_receptions)) -> test_data\n\nsum(colSums(is.na(test_data)))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 0\n```\n\n\n:::\n:::\n\n\n## Model Building -------------------------------\n\n\n::: {.cell}\n\n```{.r .cell-code}\nexp_recipe = train_data  %>%\n  recipe(log_fantasy_points ~ .,) %>%\n  update_role(c(player_id,recent_team,fantasy_points,fantasy_points_ppr,fantasy_points_target),new_role = 'ID') %>%\n  # Generally not recommended to throw out all data, but for brevity, let's remove NAs\n  step_naomit(all_numeric_predictors()) %>%\n # Remove zero variance predictors (ie. variables that contribute nothing to prediction)\n  step_zv(all_predictors()) %>%\n  step_center(all_numeric_predictors())\n\n\n#summary(exp_recipe)\n```\n:::\n\n\n# Cross-Validation + Tuning -------------\n\n## Create Model\n\nCreate specifications for elastic net model using glmnet and tune\n\n## GLM --------------------------\n\nWe'll train a general linear model (GLM). This is a type of linear regression which provides a type of built-in variable selection. The nuts and bolts of this modeling strategy are beyond the scope of this post, but for those interested in learning, there's tons of material on a concept called *regularization* upon which this strategy is built.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Specify a penalized GLM model with tuning parameters\nglm_model <- linear_reg(penalty = tune(), mixture = tune()) %>%\n  set_engine(\"glmnet\")\n\n# Define the parameter grid\nparam_grid <- grid_regular(penalty(), mixture(), levels = 10)  # Adjust levels as needed\n\n# Set up cross-validation folds\ncv_folds <- vfold_cv(train_data, v = 5)\n\n# Create a workflow\nworkflow <- workflow() %>%\n  add_recipe(exp_recipe) %>%\n  add_model(glm_model)\n\n# Tune the model\ntuned_results <- tune_grid(\n  workflow,\n  resamples = cv_folds,\n  grid = param_grid\n)\n```\n\n::: {.cell-output .cell-output-stderr}\n\n```\n→ A | warning: A correlation computation is required, but `estimate` is constant and has 0\n               standard deviation, resulting in a divide by 0 error. `NA` will be returned.\n```\n\n\n:::\n\n::: {.cell-output .cell-output-stderr}\n\n```\nThere were issues with some computations   A: x1\n```\n\n\n:::\n\n::: {.cell-output .cell-output-stderr}\n\n```\nThere were issues with some computations   A: x3\n```\n\n\n:::\n\n::: {.cell-output .cell-output-stderr}\n\n```\nThere were issues with some computations   A: x4\n```\n\n\n:::\n\n::: {.cell-output .cell-output-stderr}\n\n```\nThere were issues with some computations   A: x5\n```\n\n\n:::\n\n::: {.cell-output .cell-output-stderr}\n\n```\n\n```\n\n\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(glmnet)\n#library(lightgbm)\n#library(bonsai)\n\nglm_spec <- linear_reg(\n  penalty = tune(),     # Lambda (regularization strength)\n  mixture = tune(),    # Alpha (0 = Ridge, 1 = Lasso, values in between = Elastic Net)\n    \n) %>%\n  set_engine(\"glmnet\")\n\nglm_wflow <-\n  workflow() %>% \n  add_recipe(exp_recipe) %>%\n  add_model(glm_spec)\n\n\nwr_folds <- vfold_cv(train_data, v = 5)\n\n# Tune the hyperparameters using a grid of values\nglm_tune_results <- tune_grid(\n  glm_wflow,\n  resamples = wr_folds,\n  grid = 10   # Number of tuning combinations to evaluate\n)\n```\n\n::: {.cell-output .cell-output-stderr}\n\n```\n→ A | warning: A correlation computation is required, but `estimate` is constant and has 0\n               standard deviation, resulting in a divide by 0 error. `NA` will be returned.\n```\n\n\n:::\n\n::: {.cell-output .cell-output-stderr}\n\n```\nThere were issues with some computations   A: x1\n```\n\n\n:::\n\n::: {.cell-output .cell-output-stderr}\n\n```\nThere were issues with some computations   A: x5\nThere were issues with some computations   A: x5\n```\n\n\n:::\n\n::: {.cell-output .cell-output-stderr}\n\n```\n\n```\n\n\n:::\n:::\n\n\nTune Model w/ Cross Validation\n\nExamine Tuning Results\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Display tuning results\n\nglm_tune_results %>%\n  collect_metrics() %>%\n  filter(.metric == \"rmse\") %>%\n  arrange(mean)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 10 × 8\n    penalty mixture .metric .estimator  mean     n std_err .config              \n      <dbl>   <dbl> <chr>   <chr>      <dbl> <int>   <dbl> <chr>                \n 1 4.50e- 7   0.979 rmse    standard   0.788     5 0.00287 Preprocessor1_Model10\n 2 4.65e-10   0.533 rmse    standard   0.788     5 0.00287 Preprocessor1_Model06\n 3 2.44e- 6   0.694 rmse    standard   0.788     5 0.00287 Preprocessor1_Model07\n 4 6.17e- 5   0.113 rmse    standard   0.788     5 0.00287 Preprocessor1_Model01\n 5 1.95e- 4   0.350 rmse    standard   0.788     5 0.00287 Preprocessor1_Model04\n 6 1.03e- 8   0.278 rmse    standard   0.788     5 0.00287 Preprocessor1_Model03\n 7 3.34e- 9   0.214 rmse    standard   0.788     5 0.00288 Preprocessor1_Model02\n 8 6.59e- 3   0.738 rmse    standard   0.788     5 0.00279 Preprocessor1_Model08\n 9 1.46e- 2   0.464 rmse    standard   0.788     5 0.00277 Preprocessor1_Model05\n10 8.84e- 1   0.815 rmse    standard   0.891     5 0.00228 Preprocessor1_Model09\n```\n\n\n:::\n:::\n\n\nFind the best parameters of the group. `finalize_workflow()` will choose the model with the optimal set of hyperparameters as found in `select_best()`.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Select the best hyperparameters based on RMSE\nbest_glm <- select_best(glm_tune_results, metric = 'rmse')\n\n# Finalize the workflow with the best hyperparameters\nfinal_glm_workflow <- finalize_workflow(glm_wflow, best_glm)\n\nbest_glm\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 1 × 3\n      penalty mixture .config              \n        <dbl>   <dbl> <chr>                \n1 0.000000450   0.979 Preprocessor1_Model10\n```\n\n\n:::\n:::\n\n\n\\^ Above you'll find the optimal configuration of the model. We won't get too far into the weeds here, but will not that the penalty term is small (low regularization, high complexity) and the mixture (type of regularization) indicates more of a Ridge regression (more shrinkage, less variable elimination). \n\nBelow, we'll take the best model and fit it to the entire training data set before validating it against the test set.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Fit the finalized model on the entire training data\nfinal_glm_fit <- fit(final_glm_workflow, data = train_data)\n```\n:::\n\n\n\n## Model Evaluation/Diagnostics\n\n### Variable Importance\n\n\n\n\nNow that we have our model fit on the full training set, let's evaluate it, check it's reliability and it's compliance with key assumptions.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Make predictions on the test set and tidy\nglm_predictions <- augment(final_glm_fit, new_data = test_data) %>%\n  mutate(.pred_fp = exp(.pred) + 1,\n         .resid = fantasy_points - .pred_fp)\n\n# Evaluate the model's performance (RMSE)\nglm_metrics <- glm_predictions %>%\n  metrics(truth = fantasy_points, estimate = .pred_fp)\n\n# Print the evaluation metrics\nprint(glm_metrics)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 3 × 3\n  .metric .estimator .estimate\n  <chr>   <chr>          <dbl>\n1 rmse    standard       5.45 \n2 rsq     standard       0.160\n3 mae     standard       4.19 \n```\n\n\n:::\n\n```{.r .cell-code}\n# Distribution of predictors\nglm_predictions  %>%\n  ggplot(aes(x = .resid)) +\n    geom_histogram()\n```\n\n::: {.cell-output .cell-output-stderr}\n\n```\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n```\n\n\n:::\n\n::: {.cell-output-display}\n![](wr_fantasy_pts_files/figure-html/unnamed-chunk-19-1.png){width=672}\n:::\n\n```{.r .cell-code}\n# Residuals vs fitted\nggplot(glm_predictions,aes(x = .pred_fp, y=.resid)) +\n  geom_point() +\n  geom_hline(yintercept = 0,linetype = \"dashed\",color = \"red\")+\n  labs(title = \"Residuals vs. Fitted\", y= \"Residuals\",x = \"Fitted\")\n```\n\n::: {.cell-output-display}\n![](wr_fantasy_pts_files/figure-html/unnamed-chunk-19-2.png){width=672}\n:::\n:::\n\n\nQQ - Plot\n\n\n::: {.cell}\n\n```{.r .cell-code}\nggplot(glm_predictions,aes(sample = .resid)) +\n  geom_qq() +\n  geom_qq_line() +\n  labs(title = \"Q-Q Plot of Residuals\") +\n  theme_minimal() \n```\n\n::: {.cell-output-display}\n![](wr_fantasy_pts_files/figure-html/unnamed-chunk-20-1.png){width=672}\n:::\n:::\n\n\nThis plot is a bit indicting as we can see the points at the end of the QQPlot start to tail off, suggesting that there are some outliers in fantasy points scored or a linear model may not be the best approach for this dataset. We probably should not trust this model for valid inference and prediction and should continue to explore non-linear modeling techniques or modify this model to include interactions or transform the target variable.  \n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nfinal_glm_fit %>% \n  vip::vi() %>%\n  mutate(Importance = abs(Importance),\n         Variable = fct_reorder(Variable,Importance)) %>%\n  ggplot(aes(x = Importance, y = Variable, fill = Sign)) +\n  geom_col() +\n  scale_x_continuous(expand = c(0,0)) +\n  labs(x = \"Impact\")\n```\n\n::: {.cell-output-display}\n![](wr_fantasy_pts_files/figure-html/unnamed-chunk-21-1.png){width=672}\n:::\n:::\n\n\nI'm not personally not a huge fan of variable importance plots because they don't communicate the actual \"importance\" of the variable to the outcome of our target. In other words, they're not that useful (by themselves) for interence. They really just measure the impact of the variables on predictions. I'll take the first 2 variables on this plot as examples.\n\nIn this case, we can see that \"wopr\" is defined by \n\nWeighted Opportunity Rating - 1.5 x target_share + 0.7 x air_yards_share - a weighted average that contextualizes total fantasy usage.\n\nIt makes sense for \"wopr\" to have an impact on fantasy points scored as it directly describes fantasy opportunity for receivers. All else being equal, it's likely that increased opportunity results in increased fantasy point value and we can reasonably say something like, \"If a WR sees an increase in their opportuntiy rate due to increased skill, trust from coaches, injury to teammates, etc. we can can expect an increase in fantasy production\". However is \"receiving_fumbles_lost\", the second highest impact, reasonable? Considering that fantasy points are **deducted** when a player fumbles the ball in real life, this doesn't make much sense. We'd be hard-pressed to convince anyone to target a player who fumbles the ball often. Why is this the case?\n\nIt's likely due to an issue called collinearity. In order to fumble the ball you must first have **received it**. The players with the highest number of fumbles are likely the ones getting the ball thrown to their way the most and consequently getting increased opportunity to score more points in addition to fumbles. This is a prime example of an issue called multi-collinearity or a fancy way of saying that 2 predictors are highly correlated with each other. Below we can see \"fumbles lost\" as a function of receptions and weighted opportunity, respectfully.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nggplot(train_data, aes(x = wt_receptions, y = wt_receiving_fumbles_lost)) + \n  geom_smooth()\n```\n\n::: {.cell-output .cell-output-stderr}\n\n```\n`geom_smooth()` using method = 'gam' and formula = 'y ~ s(x, bs = \"cs\")'\n```\n\n\n:::\n\n::: {.cell-output .cell-output-stderr}\n\n```\nWarning: Removed 1136 rows containing non-finite outside the scale range\n(`stat_smooth()`).\n```\n\n\n:::\n\n::: {.cell-output-display}\n![](wr_fantasy_pts_files/figure-html/unnamed-chunk-22-1.png){width=672}\n:::\n\n```{.r .cell-code}\nggplot(train_data, aes(x = wt_wopr, y = wt_receiving_fumbles_lost)) + \n  geom_smooth()\n```\n\n::: {.cell-output .cell-output-stderr}\n\n```\n`geom_smooth()` using method = 'gam' and formula = 'y ~ s(x, bs = \"cs\")'\n```\n\n\n:::\n\n::: {.cell-output .cell-output-stderr}\n\n```\nWarning: Removed 1136 rows containing non-finite outside the scale range\n(`stat_smooth()`).\n```\n\n\n:::\n\n::: {.cell-output-display}\n![](wr_fantasy_pts_files/figure-html/unnamed-chunk-22-2.png){width=672}\n:::\n:::\n\n\n\n\nAs mentioned earlier, the regularized model handles variable selection but does not completely remove the effects of multicollinearity. We will likely need a model better suited for non-linear relationships or include more interactions to improve model performance and interpretability.\n\n## Conclusion\n\nThis was a good first step in creating a model for my fantasy football needs. There is clearly room for improvement as far as model selection goes, but it was fun to use the `slider` package for the first time and attempt to a transformed target variable. I have some ideas for additions to and subtractions from this model that will hopefully make it into another blog post. In the meantime, big shoutout to the `nflreadr` team for making this data easily available. \n\n@Manual{,\n  title = {nflreadr: Download 'nflverse' Data},\n  author = {Tan Ho and Sebastian Carl},\n  year = {2024},\n  note = {R package version 1.4.1.05, https://github.com/nflverse/nflreadr},\n  url = {https://nflreadr.nflverse.com},\n}\n\n",
    "supporting": [
      "wr_fantasy_pts_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}