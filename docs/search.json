[
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "Hello everyone! My name is Sameer Sapre and I am an analyst working in the Baseball Projects group with the Seattle Mariners.\nI am also currently a graduate student at the University of Washington studying Industrial Engineering."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Sameer’s Blog",
    "section": "",
    "text": "Predicting Wide Receiver Fantasy Points w/ slideR and Tidymodels\n\n\n\n\n\n\n\n\n\n\n\nSameer Sapre\n\n\n\n\n\n\n\n\n\n\n\n\nPredicting NFL Game Outcomes with nflfastR\n\n\n\n\n\n\nnews\n\n\ncode\n\n\nanalysis\n\n\n\n\n\n\n\n\n\nJun 30, 2024\n\n\nSameer Sapre\n\n\n\n\n\n\n\n\n\n\n\n\nData-Driven Descriptions: Using Machine Learning to Profile 2020 NBA Draft Prospects\n\n\n\n\n\n\nhoops\n\n\n\n\n\n\n\n\n\nNov 27, 2020\n\n\nSameer Sapre\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/nfl-game-prediction/index.html",
    "href": "posts/nfl-game-prediction/index.html",
    "title": "Predicting NFL Game Outcomes with nflfastR",
    "section": "",
    "text": "Hello everyone! Today I want to share a tutorial on using nflreadR to read historic NFL results and model game outcomes with tidymodels.\nFirst, a quick introduction of the R packages we’ll use. nflreadR is a part of the nflverse family of packages that easily and efficiently obtain data from NFL games. This includes past games results and statistics. In this post, we’ll be using its suite of functions to get the data we need to build a simple predictive model. We’ll also use the tidymodels package to setup our model and tidyverse for data cleaning and manipulation.\n\n# install and load packages\n#install.packages(\"nflreadr\")\nlibrary('nflreadr')\nlibrary(\"tidyverse\")\nlibrary('pROC')\nlibrary('tidymodels')"
  },
  {
    "objectID": "posts/nfl-game-prediction/index.html#intro",
    "href": "posts/nfl-game-prediction/index.html#intro",
    "title": "Predicting NFL Game Outcomes with nflfastR",
    "section": "",
    "text": "Hello everyone! Today I want to share a tutorial on using nflreadR to read historic NFL results and model game outcomes with tidymodels.\nFirst, a quick introduction of the R packages we’ll use. nflreadR is a part of the nflverse family of packages that easily and efficiently obtain data from NFL games. This includes past games results and statistics. In this post, we’ll be using its suite of functions to get the data we need to build a simple predictive model. We’ll also use the tidymodels package to setup our model and tidyverse for data cleaning and manipulation.\n\n# install and load packages\n#install.packages(\"nflreadr\")\nlibrary('nflreadr')\nlibrary(\"tidyverse\")\nlibrary('pROC')\nlibrary('tidymodels')"
  },
  {
    "objectID": "posts/nfl-game-prediction/index.html#load-data",
    "href": "posts/nfl-game-prediction/index.html#load-data",
    "title": "Predicting NFL Game Outcomes with nflfastR",
    "section": "Load Data",
    "text": "Load Data\nNow that we have the relevant packages loaded, let’s get started getting our data together. Starting with game data, we’ll pull game results from 2011 - 2021. Here, we see that we get a schedule where each row (record) represents a game. There’s a home and away team, corresponding scores, and more contextual information for each game.\n\n# Scrape schedule Results\nload_schedules(seasons = seq(2011,2024)) -&gt; nfl_game_results \nhead(nfl_game_results)\n\n# A tibble: 6 × 46\n  game_id   season game_type  week gameday weekday gametime away_team away_score\n  &lt;chr&gt;      &lt;int&gt; &lt;chr&gt;     &lt;int&gt; &lt;chr&gt;   &lt;chr&gt;   &lt;chr&gt;    &lt;chr&gt;          &lt;int&gt;\n1 2011_01_…   2011 REG           1 2011-0… Thursd… 20:30    NO                34\n2 2011_01_…   2011 REG           1 2011-0… Sunday  13:00    PIT                7\n3 2011_01_…   2011 REG           1 2011-0… Sunday  13:00    ATL               12\n4 2011_01_…   2011 REG           1 2011-0… Sunday  13:00    CIN               27\n5 2011_01_…   2011 REG           1 2011-0… Sunday  13:00    IND                7\n6 2011_01_…   2011 REG           1 2011-0… Sunday  13:00    TEN               14\n# ℹ 37 more variables: home_team &lt;chr&gt;, home_score &lt;int&gt;, location &lt;chr&gt;,\n#   result &lt;int&gt;, total &lt;int&gt;, overtime &lt;int&gt;, old_game_id &lt;chr&gt;, gsis &lt;int&gt;,\n#   nfl_detail_id &lt;chr&gt;, pfr &lt;chr&gt;, pff &lt;int&gt;, espn &lt;chr&gt;, ftn &lt;int&gt;,\n#   away_rest &lt;int&gt;, home_rest &lt;int&gt;, away_moneyline &lt;int&gt;,\n#   home_moneyline &lt;int&gt;, spread_line &lt;dbl&gt;, away_spread_odds &lt;int&gt;,\n#   home_spread_odds &lt;int&gt;, total_line &lt;dbl&gt;, under_odds &lt;int&gt;,\n#   over_odds &lt;int&gt;, div_game &lt;int&gt;, roof &lt;chr&gt;, surface &lt;chr&gt;, temp &lt;int&gt;, …\n\n\nWe’ve loaded our schedules in with some interesting variables to use in our model. However, it’s not quite in the format we need it to be. Ideally, we’d like to feed in 2 teams and have the model give us a winner.\n\nnfl_game_results %&gt;%\n  # Remove the upcoming season\n  filter(season &lt; 2024) %&gt;%\n  pivot_longer(cols = c(away_team,home_team),\n               names_to = \"home_away\",\n               values_to = \"team\") %&gt;%\n  mutate(team_score = ifelse(home_away == \"home_team\",yes = home_score,no = away_score),\n         opp_score = ifelse(home_away == \"home_team\", away_score,home_score)) %&gt;%  # sort for cumulative avg\n  arrange(season,week) %&gt;%\n  select(season,game_id,team,team_score,opp_score,week) -&gt; team_games\n\nLet’s use pivot_longer() to rearrange our dataset and select some simple variables before making the matchup set."
  },
  {
    "objectID": "posts/nfl-game-prediction/index.html#feature-engineering",
    "href": "posts/nfl-game-prediction/index.html#feature-engineering",
    "title": "Predicting NFL Game Outcomes with nflfastR",
    "section": "Feature Engineering",
    "text": "Feature Engineering\nOur goal is to be able to predict the outcome of each. To do that, we need to think about what impacts the outcome of a game before teams even take the field.\nIn the case of an NFL game it could be things like player skill level, how far a team has to travel, injuries, even the food that players ate the night before. Using nflreadr we can see that there are several variables that can potentially impact the game’s outcome from injuries to previous results.\nWe’ll start off by pulling in previous_results. By using previous results, we can hopefully capture a team’s quality as a predictor for how they will perform in the next game. There are several ways to quantify team strength, some more complex than others, but for this tutorial, we will use cumulative results as a measure of team strength. The results will be in the form of cumulative points scored/allowed and winning percentage leading up to the game.\n\nteam_games %&gt;%\n  arrange(week) %&gt;%\n  group_by(season,team) %&gt;%\n  # For each team's season calculate the cumulative scores for after each week\n  mutate(cumul_score_mean = cummean(team_score),\n          cumul_score_opp = cummean(opp_score),\n          cumul_wins = cumsum(team_score &gt; opp_score),\n          cumul_losses = cumsum(team_score &lt; opp_score),\n          cumul_ties = cumsum(team_score == opp_score),\n         cumul_win_pct = cumul_wins / (cumul_wins + cumul_losses),\n         # Create the lag variable\n         cumul_win_pct_lag_1 = lag(cumul_win_pct,1),\n         cumul_score_lag_1 = lag(cumul_score_mean,1),\n         cumul_opp_lag_1 = lag(cumul_score_opp,1)\n         ) %&gt;%\n  # Non-lag variables leak info\n  select(week,game_id,contains('lag_1')) %&gt;%\n  ungroup() -&gt; cumul_avgs\n\nLet’s also calculate winning percentage as a feature.\n\nteam_games %&gt;%\n  group_by(season,team) %&gt;%\n  summarise(wins = sum(team_score &gt; opp_score),\n            losses = sum(team_score &lt; opp_score),\n            ties = sum(team_score == opp_score))%&gt;%\n  ungroup() %&gt;%\n  arrange(season) %&gt;%\n  group_by(team) %&gt;%\n  mutate(win_pct = wins / (wins + losses),\n         lag1_win_pct = lag(win_pct,1)) %&gt;%\n  ungroup() -&gt; team_win_pct\n\nThis should be a good start, but I still feel like something is missing. Football is a dangerous game and players regularly get injured. Thankfully nflreadr provides weekly injury reports. Let’s try incorporating that into our model.\n\n# Load depth charts and injury reports\ndc = load_depth_charts(seq(2011,most_recent_season()))\ninjuries = load_injuries(seq(2011,most_recent_season()))\n\n\ninjuries %&gt;%\n  filter(report_status == \"Out\") -&gt; out_inj\n\ndc %&gt;% \n  filter(depth_team == 1) -&gt; starters\n\n# Determine roster position of injured players\nstarters %&gt;%\n  select(-c(last_name,first_name,position,full_name)) %&gt;%\n  inner_join(out_inj, by = c('season','club_code' = 'team','gsis_id','game_type','week')) -&gt; injured_starters\n\n# Number of injuries by position\ninjured_starters %&gt;%\n  group_by(season,club_code,week,position) %&gt;%\n  summarise(starters_injured = n()) %&gt;%\n  ungroup() %&gt;%\n  pivot_wider(names_from = position, names_prefix = \"injured_\",values_from = starters_injured) -&gt; injuries_position\n\nhead(injuries_position)\n\n# A tibble: 6 × 19\n  season club_code  week injured_S injured_LB injured_RB injured_T injured_C\n   &lt;dbl&gt; &lt;chr&gt;     &lt;dbl&gt;     &lt;int&gt;      &lt;int&gt;      &lt;int&gt;     &lt;int&gt;     &lt;int&gt;\n1   2011 ARI           7         1         NA         NA        NA        NA\n2   2011 ARI           8         1         NA         NA        NA        NA\n3   2011 ARI           9        NA          1          1        NA        NA\n4   2011 ARI          10         1          1         NA        NA        NA\n5   2011 ARI          11         1          1         NA        NA        NA\n6   2011 ARI          12         1          1         NA        NA        NA\n# ℹ 11 more variables: injured_DT &lt;int&gt;, injured_WR &lt;int&gt;, injured_CB &lt;int&gt;,\n#   injured_G &lt;int&gt;, injured_K &lt;int&gt;, injured_TE &lt;int&gt;, injured_QB &lt;int&gt;,\n#   injured_DE &lt;int&gt;, injured_LS &lt;int&gt;, injured_P &lt;int&gt;, injured_FB &lt;int&gt;\n\n\nAlright, now we have some flags for injured starter at each position. Next, we need to bring all of our new features together."
  },
  {
    "objectID": "posts/nfl-game-prediction/index.html#joins",
    "href": "posts/nfl-game-prediction/index.html#joins",
    "title": "Predicting NFL Game Outcomes with nflfastR",
    "section": "Joins",
    "text": "Joins\n\nnfl_game_results %&gt;%\n  inner_join(cumul_avgs, by = c('game_id','season','week','home_team' = 'team')) %&gt;%\n  inner_join(cumul_avgs, by = c('game_id','season','week','away_team' = 'team'),suffix = c('_home','_away'))-&gt; w_avgs\n\n# Check for stragglers\nnfl_game_results %&gt;%\n  anti_join(cumul_avgs, by = c('game_id','season','home_team' = 'team','week')) -&gt; unplayed_games\n\n# Join previous season's results\n#w_avgs %&gt;%\n#  left_join(team_win_pct,by = c('season','home_team' = 'team')) %&gt;%\n#  left_join(team_win_pct, by = c('away_team' = 'team','season'),suffix = c('_home','_away')) -&gt; matchups\n\n\n# Indicate whether home team won\nw_avgs %&gt;%\n  mutate(home_win = as.numeric(result &gt; 0)) -&gt; matchups\n\nNow, let’s bring in our injury data.\n\nmatchups %&gt;%\n  left_join(injuries_position,by = c('season','home_team'='club_code','week')) %&gt;%\n  left_join(injuries_position,by = c('season','away_team'='club_code','week'),suffix = c('_home','_away')) %&gt;%\n  mutate(across(starts_with('injured_'), ~replace_na(.x, 0))) -&gt; matchup_full\n\nAnd … BOOM! We have a dataset with game-by-game matchups and some features to start out. Feel free to peruse the data to find potential features to include in our Model."
  },
  {
    "objectID": "posts/nfl-game-prediction/index.html#modeling",
    "href": "posts/nfl-game-prediction/index.html#modeling",
    "title": "Predicting NFL Game Outcomes with nflfastR",
    "section": "Modeling",
    "text": "Modeling\nAhh finally, now we can get to the actual model building…. which we’ll do in about 3 lines of code.\n\nlibrary('glmnet')\n# Penalized Linear Regression\n# Mixture = 1 means pure lasso\nlr_mod &lt;- \n  logistic_reg(mixture = 0.05,penalty = 1) %&gt;% \n  set_engine(\"glmnet\")\n\nAnd that’s it! We’ll start off with a basic logistic regression. There are a few tunable (though we won’t be tuning in this post) parameters, but we’ll manually set them for this exercise.\nNext, we want to train our model with cross-validation, so that we can train and test against different samples to avoid an overfit model as much as we can.\n\n# Create folds for cross-validation\nfolds &lt;- vfold_cv(train_data)\n\nThe tidymodels workflow helps organize the steps of the model creation and consolidate model objects. We won’t go into details of workflows in this post, but there is plenty of online documentation.\n\n# create a workflow using recipe and model objects\ngame_pred_wflow &lt;- \n  workflow() %&gt;% \n  add_model(lr_mod) %&gt;% \n  add_recipe(rec_impute)\n\n\nfit_cv = game_pred_wflow %&gt;%\n  fit_resamples(folds)\n\nCheck for best model fit. It looks like using ROC-AUC and accuracy agree that the first model is the best.\n\ncollect_metrics(fit_cv)\n\n# A tibble: 3 × 6\n  .metric     .estimator  mean     n std_err .config             \n  &lt;chr&gt;       &lt;chr&gt;      &lt;dbl&gt; &lt;int&gt;   &lt;dbl&gt; &lt;chr&gt;               \n1 accuracy    binary     0.629    10 0.00854 Preprocessor1_Model1\n2 brier_class binary     0.228    10 0.00108 Preprocessor1_Model1\n3 roc_auc     binary     0.725    10 0.0122  Preprocessor1_Model1\n\n\nExtract best model fit.\n\nfinal_wf = game_pred_wflow %&gt;%\n  last_fit(splits)\n\n\nfinal_model = extract_workflow(final_wf)\n\nGet variable estimates and penalty terms.\n\nfinal_model %&gt;%\n  extract_fit_engine() %&gt;%\n  tidy() %&gt;%\n  rename(penalty = lambda)\n\n# A tibble: 3,596 × 5\n   term         step estimate penalty dev.ratio\n   &lt;chr&gt;       &lt;dbl&gt;    &lt;dbl&gt;   &lt;dbl&gt;     &lt;dbl&gt;\n 1 (Intercept)     1    0.239    3.85 -2.07e-14\n 2 (Intercept)     2    0.237    3.51  1.74e- 3\n 3 (Intercept)     3    0.234    3.20  4.75e- 3\n 4 (Intercept)     4    0.229    2.91  8.26e- 3\n 5 (Intercept)     5    0.225    2.66  1.19e- 2\n 6 (Intercept)     6    0.220    2.42  1.57e- 2\n 7 (Intercept)     7    0.215    2.20  1.95e- 2\n 8 (Intercept)     8    0.210    2.01  2.35e- 2\n 9 (Intercept)     9    0.205    1.83  2.75e- 2\n10 (Intercept)    10    0.199    1.67  3.16e- 2\n# ℹ 3,586 more rows\n\n\n\n# Align predictions to test dataset\npredicted_df = augment(final_model,test_data) \n\nNow, so we get a sample boost, let’s retrain the model on the full training sample.\n\n# Extract model specs\nfinal_model %&gt;%\n  extract_spec_parsnip() -&gt; final_specs\n\n# Update workflow object and retrain on full training set w/ same parameters\ngame_pred_wflow %&gt;%\n  update_model(spec = final_specs) %&gt;%\n  # Bind test and training data together\n  fit(data = cbind(train_data,test_data))-&gt; final_flow\n\nLet’s now save the model for future use and investigation.\n\nfinal_model %&gt;%\n  save(file = \"gamePred_model2024.rda\")\n\nWe’ll use this model to make predictions on future games and evaluate how our model performs in real-time."
  },
  {
    "objectID": "posts/archive/2020 NBA Draft Clustering.html",
    "href": "posts/archive/2020 NBA Draft Clustering.html",
    "title": "Data-Driven Descriptions: Using Machine Learning to Profile 2020 NBA Draft Prospects",
    "section": "",
    "text": "If you have ever read, listened to, or watched analysis of an NBA draft you might have heard some strange sounding phrases like “3 – and – D wing”, “rim-protector”, “pure scorer”, “raw athlete”, and “playmaker”  used to describe a player. What does that mean? It seems like these adjectives are all meant to do one thing - “profile” a player. Profiles are a quick summary or composite description of who a player is, expressing their playing style, strengths, weaknesses, role on a team, etc. From a fan’s perspective, these descriptions give us an idea of the player’s projected role on an NBA team without having to go back and watch hours of the player’s games. In addition, it might help us fans identify players that fit a need on our team. For example, if you’re a Cavs or Blazers fan, a solid perimeter defender may be what you’re looking for. If you’re a Sixers or Thunder fan, you might be interested in knowing the sharpshooters that could help your team. As a fan of both college basketball and the NBA Draft, these profiles and descriptions are intriguing, and I’d love to take a crack at developing my own. The only problem is that I haven’t watched nearly enough college hoops, nor have I been paying enough attention to this year’s prospects. Maybe I should leave the analysis to actual analysts on TV, but part of me still thinks I can come up with useful player profiles using data. In this post, I’ll attempt to address the idea of “profiling” NBA draft prospects (specifically, guards), using an unsupervised machine learning technique called hierarchical clustering and the season statistics of NCAA prospects dating back to 2011.\n \n\nWhat is Clustering?\n\nThe reason I am using hierarchical clustering is because the clusters that a player is assigned to can reveal the defining characteristics of that player. Without getting too technical, hierarchical clustering is an unsupervised machine learning technique used to divide observations in a dataset into clusters or groups based on statistical similarity. By grouping similar players together and evaluating the groups, we can generalize the qualities of players in each group. For example, one group may consist of players with a high 3-point percentage and low assist percentage revealing that they were generally effective as off-ball shooters for their college team. Of course, not all players have skill sets that can be identified with a given set of statistics or any available statistics for that matter. However, that is one of the challenges of generalizing player profiles. In this analysis, I do my best to mitigate these issues, but there are still a few players whose resulting profiles don’t make a ton of sense.\nUnlike supervised models, trying to find out if the results of a cluster analysis are “good” doesn’t come down to prediction accuracy or error, but rather how similar observations are to others in their cluster and how different they are from those outside. There are ways to validate resulting clusters, like using Silhouette scores or Dunn’s Index, that judge if the resulting groups actually contain mathematically similar observations. If this is getting too technical, don’t worry, the bottom line is that players that are grouped together should generally share more in common then players not grouped together.\nHowever, for this post, mathematically “good” results will not be prioritized over interpretability. In fact, my criteria for success is not technical at all. In order for this analysis to be a success, the resulting clusters/groups must be interpretable and must not be indicative of NBA success. That means that each group of players should have defining characteristics that can be described using basketball terminology and that each group includes players with varying levels of NBA success. Again, the goal of this analysis is build profiles that describe a player, not predict his chances of success. By the way, if you haven’t already noticed I use “cluster” and “group” interchangeably, sorry for any confusion but they mean the same thing.\n\nApproach and Data\n\nRather than using only the college statistics of the 2020 class, I am including the college stats of current NBA players to make the resulting groups more interpretable. Of course, I will be using the average statistics of each group to get a better understanding of the group’s defining traits. However, by including current NBA players in the analysis, each group’s characteristics become more recognizable. For example, any group headlined by Buddy Hield or Joe Harris could probably be identified as a group consisting of primarily sharpshooters while a group including Matisse Thybulle and Marcus Smart could be thought of as a group of good on-ball defenders. In addition, the model does not consider the year that each player was drafted meaning that Anthony Edwards, Tre Jones, and Cole Anthony will be grouped together with similar players from previous draft classes. As a result, we’ll also get an idea of each 2020 draft class member’s NBA comparisons. Of course, there are always going to be players whose college profile is different from their NBA profile, but that doesn’t seem to affect the results too much.\nIf you want to check out the technical details/data selection, it will be available on GitHub. Long story short, I used the statistics of the final college year of almost every single guard that has played in the NBA since 2011 as well as guards included in NBADraftNet.com’s 2020 rankings. Unfortunately, this analysis does not include players that did not play in the NCAA. That means no LaMelo Ball, Killian Hayes, Theo Maldeon, or RJ Hampton nor does the analysis include any current players that played overseas instead of in college. That means players like Bogdan Bogdanovic, Dennis Schroder, or Emmanuel Mudiay will be left out as well.\nNext, as un-inspiring as it sounds, I decided to hand-pick statistics that I felt were most important when trying to discern the various roles and playing styles of guards among each other. The final set of statistics I used were assist percentage (AST %), usage percentage (USG%), three-point attempt rate (3PAr), effective-field goal percentage (EFG%), and defensive box-plus-minus (DBPM).\n \n\nResults\n\nThe final model produced 6 clusters of players that generally make sense and can serve as statistical profiles. It was by no means a complete success as there were some players in questionable/interesting groups, but, as a whole, it wasn’t too difficult to come up with descriptions for each cluster using group averages and the players within them. Here is an overview of each group’s statistical profile.\n \n\nGroup 1: Low Efficiency 3 – and – D\n\nIn our first group, players tended to be solid defenders, but weren’t very efficient scorers, nor were they the primary creators for their team. The group includes notable NBA starters like Donovan Mitchell, Gary Harris, and Kentavious Caldwell-Pope, but also contained players that didn’t make much of an impact at the next level like Rawle Alkins, Aaron Harrison, and Malachi Richardson. The best-case scenarios for these guys don’t look too bad. Harris and KCP were both starters for the two Western Conference Finals teams with KCP going on to win the finals with LA as a key contributor on both ends. He was also part of one of the best defensive units in the league. It’s also surprising that Donovan Mitchell was included in this group. He has now become the offensive focal point of the Utah Jazz and an All-Star in the process of shedding this label. It’s also worth noting that all of the top 3 players are not known to be particularly lethal shooters, but tend to be streaky shooters capable of going on hot/cold stretches at a moment’s notice. Streaky shooters are notorious for their willingness to shoot despite their recent struggles. Therefore, the clustering did a good job of grouping players that will continue to exhibit a high three-point attempt rate regardless of the percentage they are shooting.\nThe 2020 prospect that fell into this group was Isaiah Joe, a 6’5 guard from Arkansas whose efficiency (49.7 EFG%) isn’t great, but he did shoot a lot of threes (76.4 % 3PTAr).\n \n Statistical profile of players in Cluster/Group 1\n\nGroup 2: Old-school floor generals\n\nFor group 2, we can find offensively efficient primary ballhandlers/creators given the groups relatively high effective field goal percentage, low usage percentage, and high assist percentage. Players in this group include Denzel Valentine, Derrick White, and Reggie Jackson as well as Scott Machado and Ray MacCallum. These players really made sense when looking at the group averages. They seemed to be making good decisions with the basketball, assisting a large amount of teammate field goals while using up a relatively small share of possessions (turnovers are also included in usage percentage). In addition, despite their lack of three-point shooting, they still shot the ball very efficiently inferring that they took smart shots and often found higher percentage looks. While no one assigned to this group is a star, there are still solid role players and starters in the NBA that carried this label in college.\nThe only 2020 prospect assigned to this group was Oregon’s Payton Pritchard. who shot threes at a decently high rate (45.9 % as a senior) along with solid efficiency numbers.\n Statistical profile of players in Cluster/Group 2\n \n \n\nGroup 3 – High Volume Scorers\n\nIn group 3, we primarily found what some might call volume scorers. These players had a high usage rate and a low assist percentage suggesting that they used a large portion of their team’s possessions to shoot or turn it over. They also carried okay scoring efficiency and subpar defense. Notable NBA players include Buddy Hield, Damian Lillard, and Jamal Murray while fringe players include Xavier Munford, Rashad Vaughn, and Gian Clavall. It’s important to note that while the statistical profiles of players in this group don’t seem great, some of them have still gone on to become solid NBA contributors. Damian Lillard has become a superstar and can get quality shots from almost anywhere on the court. Hield, despite his high volume at Oklahoma, was still a very efficient shooter (0.623 EFG%) and was a key contributor for Sacramento before issues with the coaching staff. Finally, Jamal Murray exploded onto the scene in this year’s playoffs helping Denver to the Western Conference Finals with a ridiculous 62.6 True Shooting percentage and a stretch of 3 games in which he scored a total of 142 points.\nThe 2020 prospects assigned to this group were Markus Howard (Marquette) who has a high usage rate (39.3), low DBPM (0.6) and decent efficiency (53 EFG%) and Anthony Edwards (Georgia: we’ll get to him later).\n Statistical profile of players in Cluster/Group 3\n \n \n\nGroup 4 – Focal Points (… No pun intended)\n\nGroup 4 players looked clearly like high offensive load bearers as they had high usage and assist percentages. That combination signifies that much of the offense ran through them as they worked as the primary facilitators and shot at high volumes. Also, these guards didn’t take many threes, weren’t super-efficient, nor had great defensive numbers. Notable NBA include Trae Young, DeAngelo Russell, Ja Morant, Klay Thompson, and Dejounte Murray and fringe players Walt Lemon, Milton Doyle, and Mike James. Now you may be questioning Trae Young and Klay’s inclusion in this group, but both carried high offensive loads and weren’t that efficient, the only difference is that their three-point attempt rates were very high.\nNevertheless, what you can take away from this group is that it’s best players have no issue handling the scoring and creation responsibilities at the next level. Trae Young and DeAngelo Russell are already All-Stars with high usage and assist rates while Ja Morant seems to be following a similar trajectory in Memphis. Maybe if a player like this is given the reigns on a team in need of someone to shoulder that load, they can thrive.\n2020 prospects include: Cassius Winston (Michigan State), Saben Lee (Vanderbilt), Jamil Wilson (Marquette), Cole Anthony (UNC), and Grant Riller (Charleston).\n Statistical profile of players in Cluster/Group 4\n\nGroup 5 – Lockdown Combo Guards\n\nGroup 5 consists primarily of defensive specialists who were also the primary offensive facilitators on their team. Players of this group generally have high DBPM, and high AST% while not being the most efficient shooters. Notable NBA players include Marcus Smart, De’Aaron Fox, Shai Gilgeous-Alexander, Delon Wright, Matisse Thybulle, and Malcom Brogdon while fringe players include Tyrone Wallace, Travon Duval, Troy Caupain. There seem to be players like this available all over the draft from pick # 5 (Fox, Smart) to pick # 36 (Brogdon).  I am a big fan of this group because it contains many solid, underrated players. Shai Gilge….. is a fun player to watch and might become the cornerstone for the Thunder franchise. Marcus Smart is such a good defender that there was an argument that he should’ve been the Defensive Player of the Year. Finally, 2020 prospects to watch are Devon Dotson and Malachi Flynn who seemed to fit this statistical description pretty well.\nThere were quite a few 2020 prospects assigned to this group including Ashton Hagans (Kentucky), Devon Dotson (Kansas: 4.8 DBPM), Malachi Flynn (4.1 DBPM, San Diego State), Josh Green (Arizona), Tre Jones (Duke), and Tyrese Maxey (Kentucky: 1.45 AST:TO Ratio).\n Statistical profile of players in Cluster/Group 5\n\nGroup 6 – Efficient 3 – and – D Wings\n\nFinally, players of Group 6 look like they can be true 3-and-D wings. These players had very high EFG% to go with a high 3PAr. They also carried a decent DBPM while carrying low usage and assist rates. Notable NBA players include Bradley Beal, Devin Booker, Joe Harris, Tyler Herro, Terrance Ross, as well as Lonzo Ball and Victor Oladipo (64.8% EFG, 6.2 DBPM). This may seem strange, but Lonzo Ball was very efficient as a shooter (66.8 % EFG), shot lots of 3s (56.6 % 3PAr), and was a great defender (3.9 DBPM), he just happened to also have a very high assist percentage (31.4%). Overall, these also seem to be the most “NBA ready”  players in the draft. Most of the top players of this group were starters right away. Most recently and perhaps notably, 19 year old Tyler Herro started every game for the Eastern Conference champion Miami Heat. This early success might be due to the shift of the game as a whole. As teams have started embracing the three-point shot as more of a necessity rather than an option, players who are good 3-point shooters have naturally become more valuable in today’s game.\n2020 prospects include Tyrese Haliburton (Iowa State), Tyrell Terry (Stanford: 45.6% 3Par, 20 AST%, 53.5% EFG), Immanuel Quickly (Kentucky), Desmond Bane (TCU), and Cassius Stanley (Duke).\n Statistical profile of players in Cluster/Group 6\n\nConclusion\n\nThere are a few points to mention with these results. First, It looks like there are plenty of solid defenders to be found in this upcoming draft (both primary ballhandlers and shooters) by the large representation of 2020 prospects found in clusters 5 and 6. Second, some notable players I want to analyze further include Anthony Edwards and Tyrese Haliburton.\nEdwards was placed in a group occupied primarily by volume scorers. He will be a top-3 pick, but will teams consider his lack of defensive impact (0.7 DBPM) and low efficiency (47.3% EFG)? Of course, players can improve and maybe the top of the draft is a perfect spot for high usage, low efficiency “projects”. Teams at the top of the draft may be more willing to give a longer leash to prospects and being on a bad team might give Edwards opportunities, in terms of volume, that could help him develop. Just look at fellow top-10 picks in his group – Damian Lillard, Buddy Hield, Jamal Murray, Brandon Knight, Austin Rivers. While Edwards’ future team hopes it’s not the latter two (By the way, Knight had a promising start to his career before injuries got in the way), the first three may be indicators of how his team should handle his development. For this reason, perhaps Minnesota, a team hoping to make a push for the playoffs and maximize the opportunity they have with KAT and DeAngelo Russell, should opt for someone who will not command a high volume. Honestly, the same can be said for Golden State, Edwards will likely have to cede volume to Klay, Steph, Draymond, and Andrew Wiggins and will also be expected to contribute immediately to a deep playoff push next year. I could see Charlotte, despite solid guard play from Devonte Graham and Terry Rozier this season, being a good fit for Edwards. They don’t seem close to competing for anything just yet and could be the perfect landing spot for Edwards to get the opportunities he needs to develop.\nIn addition, the inclusion of Tyrese Haliburton as a 3 – and – D wing is also interesting. He has a high assist percentage (35%) and effective field-goal percentage (61.1%) while carrying a relatively low usage rate for a point guard (20.1%) and shot about half of his shots (50.8%) from downtown. This might mean that he has a versatile skill set and could serve a team in multiple ways in the NBA. For teams that already have young point guards like Chicago or New York, Haliburton might still be a good fit for operating in some sort of hybrid role. Even teams with a perceived need for a point guard like Detroit or Phoenix, could use him as their primary ballhandler.\nIn conclusion, the groups produced by the hierarchical clustering model met the goals originally defined for them: they were interpretable, and each cluster contained players with different levels of NBA success. Of course, the results weren’t perfect as a few players seemed to be placed in groups unintuitively, but not all basketball players can be easily profiled with a small set of statistics.  Nevertheless, I hope that this post provided a new perspective on player profiling using a more statistical approach.\nAll data used for this project was obtained from Basketball-Reference.com."
  },
  {
    "objectID": "posts/post-with-code/wr_fantasy_pts.html",
    "href": "posts/post-with-code/wr_fantasy_pts.html",
    "title": "Predicting Wide Receiver Fantasy Points w/ slideR and Tidymodels",
    "section": "",
    "text": "library(tidyverse)\nlibrary(nflreadr)\nlibrary(tidymodels)\nlibrary(nflfastR)"
  },
  {
    "objectID": "posts/post-with-code/wr_fantasy_pts.html#r-markdown",
    "href": "posts/post-with-code/wr_fantasy_pts.html#r-markdown",
    "title": "Predicting Wide Receiver Fantasy Points w/ slideR and Tidymodels",
    "section": "",
    "text": "library(tidyverse)\nlibrary(nflreadr)\nlibrary(tidymodels)\nlibrary(nflfastR)"
  },
  {
    "objectID": "posts/post-with-code/wr_fantasy_pts.html#data-load",
    "href": "posts/post-with-code/wr_fantasy_pts.html#data-load",
    "title": "Predicting Wide Receiver Fantasy Points w/ slideR and Tidymodels",
    "section": "Data Load",
    "text": "Data Load\n\nstats24 = load_player_stats(2024)\nstats = load_player_stats(seasons = seq(2006,2023))\npbp23 = load_pbp(2023)\n\n\ndc = load_depth_charts(season = seq(2016,most_recent_season())) %&gt;% filter(position == 'WR',formation == 'Offense') %&gt;%\n  select(season,recent_team = club_code,week,season_type = game_type,player_id = gsis_id,depth_team)\n\n\n# Filter for wide receiver plays\nwr_data &lt;- stats %&gt;%\n  filter(position == \"WR\") %&gt;%\n  select(player_id,player_name,position,recent_team,season,week,season_type,\n         receptions:fantasy_points_ppr) %&gt;%\n  # Receiving FP\n  mutate(rec_fp = (receiving_yards * 0.1) + (receiving_tds * 6) + (receptions * 0.5)) %&gt;%\n  # Add depth chart status since we don't have participation data\n  left_join(y = dc,by = c('player_id','week','season','season_type',\"recent_team\")) %&gt;%\n  # Only first 3 are counted so some players are NA'd if they're below 3rd on DC\n  replace_na(list(depth_team = '4')) %&gt;%\n  mutate(depth_team = as.numeric(depth_team))\n\nNumber of Snaps by offense ———\nFFO (Comparison)\n\nread.csv('https://github.com/ffverse/ffopportunity/releases/download/latest-data/ep_weekly_2024.csv') -&gt; ff_2024\n\nff_2024 %&gt;% filter(position == 'WR') %&gt;% select(game_id,player_id,season,rec_fantasy_points,rec_fantasy_points_exp) %&gt;%\n  metrics(truth = rec_fantasy_points,estimate = rec_fantasy_points_exp)\n\n# A tibble: 3 × 3\n  .metric .estimator .estimate\n  &lt;chr&gt;   &lt;chr&gt;          &lt;dbl&gt;\n1 rmse    standard       4.57 \n2 rsq     standard       0.629\n3 mae     standard       3.28"
  },
  {
    "objectID": "posts/post-with-code/wr_fantasy_pts.html#missingness",
    "href": "posts/post-with-code/wr_fantasy_pts.html#missingness",
    "title": "Predicting Wide Receiver Fantasy Points w/ slideR and Tidymodels",
    "section": "Missingness",
    "text": "Missingness\nIt looks like there are some patterns of missingness in the dataset we’ve created. Ideally, we’d could probably impute these values, but for the brevity, we’ll filter out the NA’s of racr, due to the intersection with other metrics.\n\nlibrary(naniar)\n\ngg_miss_case(wr_data)\n\n\n\n\n\n\n\ngg_miss_var(wr_data)\n\n\n\n\n\n\n\nwr_data %&gt;%\n  select(racr,air_yards_share,wopr,target_share,receiving_epa) -&gt; df_miss\n\ndf_miss %&gt;%\n  gg_miss_upset()\n\n\n\n\n\n\n\n# What if we take out racr?"
  },
  {
    "objectID": "posts/post-with-code/wr_fantasy_pts.html#time-weighting",
    "href": "posts/post-with-code/wr_fantasy_pts.html#time-weighting",
    "title": "Predicting Wide Receiver Fantasy Points w/ slideR and Tidymodels",
    "section": "Time Weighting",
    "text": "Time Weighting\nWe want our predictions for each player to be based on their past performance. Ideally, we’d like the most recent performances to be weighed the heaviest. To do that, we can introduce a simple time-weighting scheme. In the function below, we take a data vector (ex. receptions) and return a weighted average based on the length of the vector.\n\n# Create Time Weighting \n\nweighted_avg = function(metric_vector){\n  \n  # Take in sliding window of vector of chosen metric\n  n = length(metric_vector)\n\n  # Create Weights for each value based on recency\n  weights = seq(1,n)\n  \n  # Calculated weighted average\n  w_avg = sum(metric_vector * weights) / sum(weights)\n  \n  return(w_avg)\n\n}\n\nNow that we have our simple time-weighting built, let’s make use of Davis Vaughn’s [slider] (https://slider.r-lib.org/articles/slider.html) package. It’s a tidy way to calculate sliding window summaries. We’ll do that to create a set of potential numeric predictors.\n\nlibrary(TTR)\n\n\nAttaching package: 'TTR'\n\n\nThe following object is masked from 'package:dials':\n\n    momentum\n\nlibrary(slider)\n\nwr_data %&gt;%\n  # Should remove most missingness\n  filter(!is.na(racr)) %&gt;%\n  group_by(player_id,season) %&gt;%\n  arrange(week) %&gt;%\n  # Weighted Avg (moving)\n  # Take lag so we are not leaking any data\n  mutate(across(receptions:depth_team, ~ lag(slide_dbl(.x,.f = weighted_avg,.before = Inf,.complete = TRUE)),.names = \"wt_{col}\")) %&gt;%\n  ungroup() %&gt;%\n  # Convert negative fantasy points to 0\n  mutate(fantasy_points_target = ifelse(fantasy_points &lt; 0,0,fantasy_points),\n         log_fantasy_points = log(fantasy_points_target + 1)) %&gt;%\n  # Need data, don't use week 1 for now\n  filter(week &gt; 1) -&gt; ma_wr"
  },
  {
    "objectID": "posts/post-with-code/wr_fantasy_pts.html#correlations",
    "href": "posts/post-with-code/wr_fantasy_pts.html#correlations",
    "title": "Predicting Wide Receiver Fantasy Points w/ slideR and Tidymodels",
    "section": "Correlations",
    "text": "Correlations\nBefore we start splitting data for the model, let’s take a look at variable correlations. This may influence our model choices.\n\n# Calculate the correlation matrix\ncor_matrix &lt;- ma_wr %&gt;% \n  select(starts_with(\"wt_\"), fantasy_points_target) %&gt;%\n  cor(use = \"complete.obs\")\n\n# Reshape the correlation matrix for ggplot\ncor_data &lt;- reshape2::melt(cor_matrix)\n\nggplot(data = cor_data, aes(x = Var1, y = Var2, fill = value)) +\n  geom_tile(color = \"white\") +\n  scale_fill_gradient2(low = \"blue\", high = \"red\", mid = \"white\", midpoint = 0, limit = c(-1,1), name=\"Correlation\") +\n  theme_minimal() +\n  theme(axis.text.x = element_text(angle = 45, vjust = 1, hjust = 1)) +\n  labs(title = \"Correlation Matrix of Dataset\", x = \"\", y = \"\")\n\n\n\n\n\n\n\n\nOkay, it looks like there is a high degree of colinearity between our weighted measures. Not very surprising. We’ll use a few modeling techniques below that should address the issues of highly correlated variables.\nIn addition, if we look at our target variable, we notice something interesting… it’s got a funky distribution. Should we be using linear regression?\n\nma_wr %&gt;%\n  ggplot(aes(x = fantasy_points_target)) +\n    geom_histogram()\n\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n\n\n\n\n\n\n\n\nma_wr %&gt;%\n  ggplot(aes(x = log_fantasy_points)) +\n  geom_histogram()\n\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`."
  },
  {
    "objectID": "posts/post-with-code/wr_fantasy_pts.html#preprocess",
    "href": "posts/post-with-code/wr_fantasy_pts.html#preprocess",
    "title": "Predicting Wide Receiver Fantasy Points w/ slideR and Tidymodels",
    "section": "Preprocess",
    "text": "Preprocess\n\nlibrary(tidymodels)\n\n# Split\n\nset.seed(222)\n# Put 3/4 of the data into the training set \ndata_split &lt;- ma_wr %&gt;% \n  # Don't use first week\n  filter(week &gt;1)%&gt;%\n  # Filter on relevant columns\n  select(starts_with('wt_'),fantasy_points_target,player_id,season,week,recent_team,\n         fantasy_points,fantasy_points_ppr,log_fantasy_points) %&gt;% \n  # make split\n  initial_split( prop = 3/4)\n\n# Create data frames for the two sets:\ntrain_data &lt;- training(data_split)\ntest_data  &lt;- testing(data_split)\n\n\nwt_recipe = train_data %&gt;%\n  recipe(fantasy_points_target ~ .,) %&gt;%\n  update_role(c(player_id,recent_team,fantasy_points,fantasy_points_ppr),new_role = 'ID') %&gt;%\n  # Generally not recommended to throw out all data, but for brevity, let's remove NAs\n  step_naomit(all_numeric_predictors()) %&gt;%\n  # Remove zero variance predictors (ie. variables that contribute nothing to prediction)\n step_center(all_numeric_predictors())\n#\n\nsummary(wt_recipe)\n\n# A tibble: 28 × 4\n   variable                       type      role      source  \n   &lt;chr&gt;                          &lt;list&gt;    &lt;chr&gt;     &lt;chr&gt;   \n 1 wt_receptions                  &lt;chr [2]&gt; predictor original\n 2 wt_targets                     &lt;chr [2]&gt; predictor original\n 3 wt_receiving_yards             &lt;chr [2]&gt; predictor original\n 4 wt_receiving_tds               &lt;chr [2]&gt; predictor original\n 5 wt_receiving_fumbles           &lt;chr [2]&gt; predictor original\n 6 wt_receiving_fumbles_lost      &lt;chr [2]&gt; predictor original\n 7 wt_receiving_air_yards         &lt;chr [2]&gt; predictor original\n 8 wt_receiving_yards_after_catch &lt;chr [2]&gt; predictor original\n 9 wt_receiving_first_downs       &lt;chr [2]&gt; predictor original\n10 wt_receiving_epa               &lt;chr [2]&gt; predictor original\n# ℹ 18 more rows\n\n\n\ntest_data %&gt;%\n  filter(!is.na(wt_receptions)) -&gt; test_data\n\nsum(colSums(is.na(test_data)))\n\n[1] 0"
  },
  {
    "objectID": "posts/post-with-code/wr_fantasy_pts.html#model-building--",
    "href": "posts/post-with-code/wr_fantasy_pts.html#model-building--",
    "title": "Predicting Wide Receiver Fantasy Points w/ slideR and Tidymodels",
    "section": "Model Building ——————————-",
    "text": "Model Building ——————————-\n\nexp_recipe = train_data  %&gt;%\n  recipe(log_fantasy_points ~ .,) %&gt;%\n  update_role(c(player_id,recent_team,fantasy_points,fantasy_points_ppr,fantasy_points_target),new_role = 'ID') %&gt;%\n  # Generally not recommended to throw out all data, but for brevity, let's remove NAs\n  step_naomit(all_numeric_predictors()) %&gt;%\n # Remove zero variance predictors (ie. variables that contribute nothing to prediction)\n  step_zv(all_predictors()) %&gt;%\n  step_center(all_numeric_predictors())\n\n\n#summary(exp_recipe)"
  },
  {
    "objectID": "posts/post-with-code/wr_fantasy_pts.html#create-model",
    "href": "posts/post-with-code/wr_fantasy_pts.html#create-model",
    "title": "Predicting Wide Receiver Fantasy Points w/ slideR and Tidymodels",
    "section": "Create Model",
    "text": "Create Model\nCreate specifications for elastic net model using glmnet and tune"
  },
  {
    "objectID": "posts/post-with-code/wr_fantasy_pts.html#glm",
    "href": "posts/post-with-code/wr_fantasy_pts.html#glm",
    "title": "Predicting Wide Receiver Fantasy Points w/ slideR and Tidymodels",
    "section": "GLM ————————–",
    "text": "GLM ————————–\nWe’ll train a general linear model (GLM). This is a type of linear regression which provides a type of built-in variable selection. The nuts and bolts of this modeling strategy are beyond the scope of this post, but for those interested in learning, there’s tons of material on a concept called regularization upon which this strategy is built.\n\n# Specify a penalized GLM model with tuning parameters\nglm_model &lt;- linear_reg(penalty = tune(), mixture = tune()) %&gt;%\n  set_engine(\"glmnet\")\n\n# Define the parameter grid\nparam_grid &lt;- grid_regular(penalty(), mixture(), levels = 10)  # Adjust levels as needed\n\n# Set up cross-validation folds\ncv_folds &lt;- vfold_cv(train_data, v = 5)\n\n# Create a workflow\nworkflow &lt;- workflow() %&gt;%\n  add_recipe(exp_recipe) %&gt;%\n  add_model(glm_model)\n\n# Tune the model\ntuned_results &lt;- tune_grid(\n  workflow,\n  resamples = cv_folds,\n  grid = param_grid\n)\n\n→ A | warning: A correlation computation is required, but `estimate` is constant and has 0\n               standard deviation, resulting in a divide by 0 error. `NA` will be returned.\n\n\nThere were issues with some computations   A: x1\n\n\nThere were issues with some computations   A: x3\n\n\nThere were issues with some computations   A: x4\n\n\nThere were issues with some computations   A: x5\n\n\n\n\n\n\nlibrary(glmnet)\n#library(lightgbm)\n#library(bonsai)\n\nglm_spec &lt;- linear_reg(\n  penalty = tune(),     # Lambda (regularization strength)\n  mixture = tune(),    # Alpha (0 = Ridge, 1 = Lasso, values in between = Elastic Net)\n    \n) %&gt;%\n  set_engine(\"glmnet\")\n\nglm_wflow &lt;-\n  workflow() %&gt;% \n  add_recipe(exp_recipe) %&gt;%\n  add_model(glm_spec)\n\n\nwr_folds &lt;- vfold_cv(train_data, v = 5)\n\n# Tune the hyperparameters using a grid of values\nglm_tune_results &lt;- tune_grid(\n  glm_wflow,\n  resamples = wr_folds,\n  grid = 10   # Number of tuning combinations to evaluate\n)\n\n→ A | warning: A correlation computation is required, but `estimate` is constant and has 0\n               standard deviation, resulting in a divide by 0 error. `NA` will be returned.\n\n\nThere were issues with some computations   A: x1\n\n\nThere were issues with some computations   A: x5\nThere were issues with some computations   A: x5\n\n\n\n\n\nTune Model w/ Cross Validation\nExamine Tuning Results\n\n# Display tuning results\n\nglm_tune_results %&gt;%\n  collect_metrics() %&gt;%\n  filter(.metric == \"rmse\") %&gt;%\n  arrange(mean)\n\n# A tibble: 10 × 8\n    penalty mixture .metric .estimator  mean     n std_err .config              \n      &lt;dbl&gt;   &lt;dbl&gt; &lt;chr&gt;   &lt;chr&gt;      &lt;dbl&gt; &lt;int&gt;   &lt;dbl&gt; &lt;chr&gt;                \n 1 4.50e- 7   0.979 rmse    standard   0.788     5 0.00287 Preprocessor1_Model10\n 2 4.65e-10   0.533 rmse    standard   0.788     5 0.00287 Preprocessor1_Model06\n 3 2.44e- 6   0.694 rmse    standard   0.788     5 0.00287 Preprocessor1_Model07\n 4 6.17e- 5   0.113 rmse    standard   0.788     5 0.00287 Preprocessor1_Model01\n 5 1.95e- 4   0.350 rmse    standard   0.788     5 0.00287 Preprocessor1_Model04\n 6 1.03e- 8   0.278 rmse    standard   0.788     5 0.00287 Preprocessor1_Model03\n 7 3.34e- 9   0.214 rmse    standard   0.788     5 0.00288 Preprocessor1_Model02\n 8 6.59e- 3   0.738 rmse    standard   0.788     5 0.00279 Preprocessor1_Model08\n 9 1.46e- 2   0.464 rmse    standard   0.788     5 0.00277 Preprocessor1_Model05\n10 8.84e- 1   0.815 rmse    standard   0.891     5 0.00228 Preprocessor1_Model09\n\n\nFind the best parameters of the group. finalize_workflow() will choose the model with the optimal set of hyperparameters as found in select_best().\n\n# Select the best hyperparameters based on RMSE\nbest_glm &lt;- select_best(glm_tune_results, metric = 'rmse')\n\n# Finalize the workflow with the best hyperparameters\nfinal_glm_workflow &lt;- finalize_workflow(glm_wflow, best_glm)\n\nbest_glm\n\n# A tibble: 1 × 3\n      penalty mixture .config              \n        &lt;dbl&gt;   &lt;dbl&gt; &lt;chr&gt;                \n1 0.000000450   0.979 Preprocessor1_Model10\n\n\n^ Above you’ll find the optimal configuration of the model. We won’t get too far into the weeds here, but will not that the penalty term is small (low regularization, high complexity) and the mixture (type of regularization) indicates more of a Ridge regression (more shrinkage, less variable elimination).\nBelow, we’ll take the best model and fit it to the entire training data set before validating it against the test set.\n\n# Fit the finalized model on the entire training data\nfinal_glm_fit &lt;- fit(final_glm_workflow, data = train_data)"
  },
  {
    "objectID": "posts/post-with-code/wr_fantasy_pts.html#model-evaluationdiagnostics",
    "href": "posts/post-with-code/wr_fantasy_pts.html#model-evaluationdiagnostics",
    "title": "Predicting Wide Receiver Fantasy Points w/ slideR and Tidymodels",
    "section": "Model Evaluation/Diagnostics",
    "text": "Model Evaluation/Diagnostics\n\nVariable Importance\nNow that we have our model fit on the full training set, let’s evaluate it, check it’s reliability and it’s compliance with key assumptions.\n\n# Make predictions on the test set and tidy\nglm_predictions &lt;- augment(final_glm_fit, new_data = test_data) %&gt;%\n  mutate(.pred_fp = exp(.pred) + 1,\n         .resid = fantasy_points - .pred_fp)\n\n# Evaluate the model's performance (RMSE)\nglm_metrics &lt;- glm_predictions %&gt;%\n  metrics(truth = fantasy_points, estimate = .pred_fp)\n\n# Print the evaluation metrics\nprint(glm_metrics)\n\n# A tibble: 3 × 3\n  .metric .estimator .estimate\n  &lt;chr&gt;   &lt;chr&gt;          &lt;dbl&gt;\n1 rmse    standard       5.45 \n2 rsq     standard       0.160\n3 mae     standard       4.19 \n\n# Distribution of predictors\nglm_predictions  %&gt;%\n  ggplot(aes(x = .resid)) +\n    geom_histogram()\n\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n\n\n\n\n\n\n\n\n# Residuals vs fitted\nggplot(glm_predictions,aes(x = .pred_fp, y=.resid)) +\n  geom_point() +\n  geom_hline(yintercept = 0,linetype = \"dashed\",color = \"red\")+\n  labs(title = \"Residuals vs. Fitted\", y= \"Residuals\",x = \"Fitted\")\n\n\n\n\n\n\n\n\nQQ - Plot\n\nggplot(glm_predictions,aes(sample = .resid)) +\n  geom_qq() +\n  geom_qq_line() +\n  labs(title = \"Q-Q Plot of Residuals\") +\n  theme_minimal() \n\n\n\n\n\n\n\n\nThis plot is a bit indicting as we can see the points at the end of the QQPlot start to tail off, suggesting that there are some outliers in fantasy points scored or a linear model may not be the best approach for this dataset. We probably should not trust this model for valid inference and prediction and should continue to explore non-linear modeling techniques or modify this model to include interactions or transform the target variable.\n\nfinal_glm_fit %&gt;% \n  vip::vi() %&gt;%\n  mutate(Importance = abs(Importance),\n         Variable = fct_reorder(Variable,Importance)) %&gt;%\n  ggplot(aes(x = Importance, y = Variable, fill = Sign)) +\n  geom_col() +\n  scale_x_continuous(expand = c(0,0)) +\n  labs(x = \"Impact\")\n\n\n\n\n\n\n\n\nI’m not personally not a huge fan of variable importance plots because they don’t communicate the actual “importance” of the variable to the outcome of our target. In other words, they’re not that useful (by themselves) for interence. They really just measure the impact of the variables on predictions. I’ll take the first 2 variables on this plot as examples.\nIn this case, we can see that “wopr” is defined by\nWeighted Opportunity Rating - 1.5 x target_share + 0.7 x air_yards_share - a weighted average that contextualizes total fantasy usage.\nIt makes sense for “wopr” to have an impact on fantasy points scored as it directly describes fantasy opportunity for receivers. All else being equal, it’s likely that increased opportunity results in increased fantasy point value and we can reasonably say something like, “If a WR sees an increase in their opportuntiy rate due to increased skill, trust from coaches, injury to teammates, etc. we can can expect an increase in fantasy production”. However is “receiving_fumbles_lost”, the second highest impact, reasonable? Considering that fantasy points are deducted when a player fumbles the ball in real life, this doesn’t make much sense. We’d be hard-pressed to convince anyone to target a player who fumbles the ball often. Why is this the case?\nIt’s likely due to an issue called collinearity. In order to fumble the ball you must first have received it. The players with the highest number of fumbles are likely the ones getting the ball thrown to their way the most and consequently getting increased opportunity to score more points in addition to fumbles. This is a prime example of an issue called multi-collinearity or a fancy way of saying that 2 predictors are highly correlated with each other. Below we can see “fumbles lost” as a function of receptions and weighted opportunity, respectfully.\n\nggplot(train_data, aes(x = wt_receptions, y = wt_receiving_fumbles_lost)) + \n  geom_smooth()\n\n`geom_smooth()` using method = 'gam' and formula = 'y ~ s(x, bs = \"cs\")'\n\n\nWarning: Removed 1136 rows containing non-finite outside the scale range\n(`stat_smooth()`).\n\n\n\n\n\n\n\n\nggplot(train_data, aes(x = wt_wopr, y = wt_receiving_fumbles_lost)) + \n  geom_smooth()\n\n`geom_smooth()` using method = 'gam' and formula = 'y ~ s(x, bs = \"cs\")'\n\n\nWarning: Removed 1136 rows containing non-finite outside the scale range\n(`stat_smooth()`).\n\n\n\n\n\n\n\n\n\nAs mentioned earlier, the regularized model handles variable selection but does not completely remove the effects of multicollinearity. We will likely need a model better suited for non-linear relationships or include more interactions to improve model performance and interpretability."
  },
  {
    "objectID": "posts/post-with-code/wr_fantasy_pts.html#conclusion",
    "href": "posts/post-with-code/wr_fantasy_pts.html#conclusion",
    "title": "Predicting Wide Receiver Fantasy Points w/ slideR and Tidymodels",
    "section": "Conclusion",
    "text": "Conclusion\nThis was a good first step in creating a model for my fantasy football needs. There is clearly room for improvement as far as model selection goes, but it was fun to use the slider package for the first time and attempt to a transformed target variable. I have some ideas for additions to and subtractions from this model that will hopefully make it into another blog post. In the meantime, big shoutout to the nflreadr team for making this data easily available.\n@Manual{, title = {nflreadr: Download ‘nflverse’ Data}, author = {Tan Ho and Sebastian Carl}, year = {2024}, note = {R package version 1.4.1.05, https://github.com/nflverse/nflreadr}, url = {https://nflreadr.nflverse.com}, }"
  }
]