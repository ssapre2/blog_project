---
title: "Predicting Wide Receiver Fantasy Points w/ slideR and Tidymodels"
author: "Sameer Sapre"
editor: visual
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## R Markdown

```{r message=TRUE, warning=TRUE}
library(tidyverse)
library(nflreadr)
library(tidymodels)
library(nflfastR)
```

## Data Load

```{r message=TRUE, warning=TRUE}
stats24 = load_player_stats(2024)
stats = load_player_stats(seasons = seq(2006,2023))
pbp23 = load_pbp(2023)


dc = load_depth_charts(season = seq(2016,most_recent_season())) %>% filter(position == 'WR',formation == 'Offense') %>%
  select(season,recent_team = club_code,week,season_type = game_type,player_id = gsis_id,depth_team)

```

```{r}
# Filter for wide receiver plays
wr_data <- stats %>%
  filter(position == "WR") %>%
  select(player_id,player_name,position,recent_team,season,week,season_type,
         receptions:fantasy_points_ppr) %>%
  # Receiving FP
  mutate(rec_fp = (receiving_yards * 0.1) + (receiving_tds * 6) + (receptions * 0.5)) %>%
  # Add depth chart status since we don't have participation data
  left_join(y = dc,by = c('player_id','week','season','season_type',"recent_team")) %>%
  # Only first 3 are counted so some players are NA'd if they're below 3rd on DC
  replace_na(list(depth_team = '4')) %>%
  mutate(depth_team = as.numeric(depth_team))

  
```

Number of Snaps by offense ---------

FFO (Comparison)

```{r}

read.csv('https://github.com/ffverse/ffopportunity/releases/download/latest-data/ep_weekly_2024.csv') -> ff_2024

ff_2024 %>% filter(position == 'WR') %>% select(game_id,player_id,season,rec_fantasy_points,rec_fantasy_points_exp) %>%
  metrics(truth = rec_fantasy_points,estimate = rec_fantasy_points_exp)

```

## Missingness

It looks like there are some patterns of missingness in the dataset we've created. Ideally, we'd could probably impute these values, but for the brevity, we'll filter out the NA's of racr, due to the intersection with other metrics. 

```{r}
library(naniar)

gg_miss_case(wr_data)

gg_miss_var(wr_data)


wr_data %>%
  select(racr,air_yards_share,wopr,target_share,receiving_epa) -> df_miss

df_miss %>%
  gg_miss_upset()
# What if we take out racr?
```


## Time Weighting

We want our predictions for each player to be based on their past performance. Ideally, we'd like the most recent performances to be weighed the heaviest. To do that, we can introduce a simple time-weighting scheme. In the function below, we take a data vector (ex. receptions) and return a weighted average based on the length of the vector.


```{r}
# Create Time Weighting 

weighted_avg = function(metric_vector){
  
  # Take in sliding window of vector of chosen metric
  n = length(metric_vector)

  # Create Weights for each value based on recency
  weights = seq(1,n)
  
  # Calculated weighted average
  w_avg = sum(metric_vector * weights) / sum(weights)
  
  return(w_avg)

}


```

Now that we have our simple time-weighting built, let's make use of Davis Vaughn's [slider] (https://slider.r-lib.org/articles/slider.html) package. It's a tidy way to calculate sliding window summaries. We'll do that to create a set of potential numeric predictors.

```{r}
library(TTR)
library(slider)

wr_data %>%
  # Should remove most missingness
  filter(!is.na(racr)) %>%
  group_by(player_id,season) %>%
  arrange(week) %>%
  # Weighted Avg (moving)
  # Take lag so we are not leaking any data
  mutate(across(receptions:depth_team, ~ lag(slide_dbl(.x,.f = weighted_avg,.before = Inf,.complete = TRUE)),.names = "wt_{col}")) %>%
  ungroup() %>%
  # Convert negative fantasy points to 0
  mutate(fantasy_points_target = ifelse(fantasy_points < 0,0,fantasy_points),
         log_fantasy_points = log(fantasy_points_target + 1)) %>%
  # Need data, don't use week 1 for now
  filter(week > 1) -> ma_wr

```



## Correlations

Before we start splitting data for the model, let's take a look at variable correlations. This may influence our model choices.

```{r}
# Calculate the correlation matrix
cor_matrix <- ma_wr %>% 
  select(starts_with("wt_"), fantasy_points_target) %>%
  cor(use = "complete.obs")

# Reshape the correlation matrix for ggplot
cor_data <- reshape2::melt(cor_matrix)

ggplot(data = cor_data, aes(x = Var1, y = Var2, fill = value)) +
  geom_tile(color = "white") +
  scale_fill_gradient2(low = "blue", high = "red", mid = "white", midpoint = 0, limit = c(-1,1), name="Correlation") +
  theme_minimal() +
  theme(axis.text.x = element_text(angle = 45, vjust = 1, hjust = 1)) +
  labs(title = "Correlation Matrix of Dataset", x = "", y = "")


```

Okay, it looks like there is a high degree of colinearity between our weighted measures. Not very surprising. We'll use a few modeling techniques below that should address the issues of highly correlated variables.


In addition, if we look at our target variable, we notice something interesting... it's got a funky distribution. Should we be using linear regression?  

```{r}
ma_wr %>%
  ggplot(aes(x = fantasy_points_target)) +
    geom_histogram()

ma_wr %>%
  ggplot(aes(x = log_fantasy_points)) +
  geom_histogram()
```



## Preprocess

```{r}
library(tidymodels)

# Split

set.seed(222)
# Put 3/4 of the data into the training set 
data_split <- ma_wr %>% 
  # Don't use first week
  filter(week >1)%>%
  # Filter on relevant columns
  select(starts_with('wt_'),fantasy_points_target,player_id,season,week,recent_team,
         fantasy_points,fantasy_points_ppr) %>% 
  # make split
  initial_split( prop = 3/4)

# Create data frames for the two sets:
train_data <- training(data_split)
test_data  <- testing(data_split)
```

```{r}
wt_recipe = train_data %>%
  recipe(fantasy_points_target ~ .,) %>%
  update_role(c(player_id,recent_team,fantasy_points,fantasy_points_ppr),new_role = 'ID') %>%
  # Generally not recommended to throw out all data, but for brevity, let's remove NAs
  step_naomit(all_numeric_predictors()) %>%
  # Remove zero variance predictors (ie. variables that contribute nothing to prediction)
 step_center(all_numeric_predictors())
#

summary(wt_recipe)
```

```{r}
test_data %>%
  filter(!is.na(wt_receptions)) -> test_data

sum(colSums(is.na(test_data)))
```

## Model Building -------------------------------

```{r}

exp_recipe = train_data  %>%
  recipe(fantasy_points_target ~ .,) %>%
  update_role(c(player_id,recent_team,fantasy_points,fantasy_points_ppr),new_role = 'ID') %>%
  # Generally not recommended to throw out all data, but for brevity, let's remove NAs
  step_naomit(all_numeric_predictors()) %>%
 # Remove zero variance predictors (ie. variables that contribute nothing to prediction)
  step_zv(all_predictors()) %>%
  step_center(all_numeric_predictors())


#summary(exp_recipe)

```

# Cross-Validation + Tuning -------------

## Create Model

Create specifications for elastic net model using glmnet and tune

## GLM --------------------------

```{r}
# Specify a penalized GLM model with tuning parameters
glm_model <- linear_reg(penalty = tune(), mixture = tune()) %>%
  set_engine("glmnet")

# Define the parameter grid
param_grid <- grid_regular(penalty(), mixture(), levels = 10)  # Adjust levels as needed

# Set up cross-validation folds
cv_folds <- vfold_cv(train_data, v = 5)

# Create a workflow
workflow <- workflow() %>%
  add_recipe(exp_recipe) %>%
  add_model(glm_model)

# Tune the model
tuned_results <- tune_grid(
  workflow,
  resamples = cv_folds,
  grid = param_grid
)


```

```{r}
library(glmnet)
#library(lightgbm)
#library(bonsai)

glm_spec <- linear_reg(
  penalty = tune(),     # Lambda (regularization strength)
  mixture = tune(),    # Alpha (0 = Ridge, 1 = Lasso, values in between = Elastic Net)
    
) %>%
  set_engine("glmnet")

glm_wflow <-
  workflow() %>% 
  add_recipe(exp_recipe) %>%
  add_model(glm_spec)


wr_folds <- vfold_cv(train_data, v = 5)

# Tune the hyperparameters using a grid of values
glm_tune_results <- tune_grid(
  glm_wflow,
  resamples = wr_folds,
  grid = 10   # Number of tuning combinations to evaluate
)
```

Tune Model w/ Cross Validation

Examine Tuning Results

```{r}
# Show the tuning results
tuned_results %>%
  collect_metrics() %>%
  filter(.metric == "rmse") %>%
  arrange(mean)

glm_tune_results %>%
  collect_metrics() %>%
  filter(.metric == "rmse") %>%
  arrange(mean)
```

Find the best parameters of the group

```{r}
# Select the best hyperparameters based on RMSE
best_glm <- select_best(glm_tune_results, metric = 'rmse')

# Finalize the workflow with the best hyperparameters
final_glm_workflow <- finalize_workflow(glm_wflow, best_glm)

best_glm
```

\^ Above you'll find the optimal configuration of the model.

```{r}
# Fit the finalized model on the entire training data
final_glm_fit <- fit(final_glm_workflow, data = train_data)

```

Predict on test data

```{r}
# Make predictions on the test set
glm_predictions <- augment(final_glm_fit, new_data = test_data)

# Evaluate the model's performance (RMSE)
glm_metrics <- glm_predictions %>%
  metrics(truth = fantasy_points_target, estimate = .pred)

# Print the evaluation metrics
print(glm_metrics)

glm_predictions %>%
  mutate(resid = log_fantasy_points - .pred) %>%
  ggplot(aes(x = resid)) +
    geom_histogram()

# Plot raw estimates
glm_predictions %>%
  mutate(fp_pred = exp(.pred) - 1, fp_resid = .pred - fantasy_points) %>%
  #filter(fantasy_points >= 0) %>%
  metrics(truth = fantasy_points,estimate = fp_pred)

glm_predictions %>%
  ggplot(aes(x = log_fantasy_points)) +
    geom_histogram()
```
